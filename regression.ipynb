{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a97e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from akshare.utils import demjson\n",
    "from akshare.utils.tqdm import get_tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from akshare.tool.trade_date_hist import tool_trade_date_hist_sina\n",
    "\n",
    "# ===== 1. 数据预处理 ===== \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch  import amp\n",
    "from torch.utils.data  import Dataset, DataLoader\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "# import py_mini_racer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b676539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近交易日：2025-09-15\n"
     ]
    }
   ],
   "source": [
    "# def is_trading_day():\n",
    "#     today = datetime.now().strftime('%Y-%m-%d') \n",
    "#     # 获取交易日历（新浪财经）\n",
    "#     trade_date_df = tool_trade_date_hist_sina() \n",
    "#     # 判断今天是否在交易日历中\n",
    "#     return today in trade_date_df['trade_date'].values\n",
    "\n",
    "# if is_trading_day():\n",
    "#     print(\"今天是交易日\")\n",
    "# else:\n",
    "#     print(\"今天不是交易日\")\n",
    "\n",
    "# is_trading_day = is_trading_day()\n",
    "# today = datetime.now().date()\n",
    "trade_date_df = tool_trade_date_hist_sina() \n",
    " \n",
    "# 确保日期列是字符串格式\n",
    "trade_date_list = trade_date_df['trade_date'].astype(str).tolist()\n",
    " \n",
    "# 获取当前日期\n",
    "today = datetime.now().date() \n",
    "lastday = today - timedelta(days=1)\n",
    "# 找到最近交易日\n",
    "recent_trade_date = today \n",
    "while str(recent_trade_date) not in trade_date_list:\n",
    "    recent_trade_date -= timedelta(days=1)\n",
    "    lastday -= timedelta(days=1)\n",
    "while str(lastday) not in trade_date_list:\n",
    "    lastday -= timedelta(days=1)\n",
    " \n",
    "print(f\"最近交易日：{recent_trade_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe3a45fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.date'>\n"
     ]
    }
   ],
   "source": [
    "print(type(recent_trade_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de3500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "'NoneType' object has no attribute 'find'  wrong data: https://d.10jqka.com.cn/v4/line/bk_881156/01/2025.js http://q.10jqka.com.cn/thshy/detail/code/881138/          date      code      open      high       low     close       volume  \\\n",
      "0  2025-01-02  881156.0  2221.520  2224.233  2091.513  2110.404  479628840.0   \n",
      "1  2025-01-03  881156.0  2108.265  2111.887  2039.321  2043.119  405636270.0   \n",
      "2  2025-01-06  881156.0  2039.098  2043.666  1999.205  2008.669  348719560.0   \n",
      "3  2025-01-07  881156.0  2010.410  2021.026  1990.481  2019.256  273896540.0   \n",
      "4  2025-01-08  881156.0  2018.007  2041.064  1983.092  2026.934  330434890.0   \n",
      "\n",
      "         amount  \n",
      "0  1.006006e+10  \n",
      "1  6.526986e+09  \n",
      "2  5.031152e+09  \n",
      "3  4.438087e+09  \n",
      "4  5.483813e+09  \n"
     ]
    }
   ],
   "source": [
    "start_year = \"2025\"\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# js_code = py_mini_racer.MiniRacer()\n",
    "# setting_file_path = \"./ths.js\"\n",
    "# with open(setting_file_path, encoding=\"utf-8\") as f:\n",
    "#     js_content = f.read()\n",
    "# js_code.eval(js_content)\n",
    "# v_code = js_code.call(\"v\")\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "        # \"Cookie\": f\"v={v_code}\",\n",
    "    }\n",
    "headers_bk = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "            \"Referer\": \"https://q.10jqka.com.cn\",\n",
    "            \"Host\": \"d.10jqka.com.cn\",\n",
    "            # \"Cookie\": f\"v={v_code}\",\n",
    "        }\n",
    "\n",
    "link_code_ls = []\n",
    "bk_data_ls = []\n",
    "code_ls = []\n",
    "\n",
    "for suffix in [\"gn/\",\"thshy/\"]:\n",
    "    url = f\"https://q.10jqka.com.cn/{suffix}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, features=\"lxml\")\n",
    "    divs = soup.find_all(name=\"div\", attrs={\"class\": \"cate_items\"})\n",
    "    for div in divs:\n",
    "        # 在每个div中查找所有的a标签\n",
    "        links = div.find_all('a') \n",
    "        for link in links:\n",
    "            # 获取href属性\n",
    "            href = link.get('href') \n",
    "            # 获取文本内容\n",
    "            text = link.text \n",
    "            link_code_ls.append((href,  text))\n",
    "\n",
    "print(len(link_code_ls))\n",
    "for symbol_url, name in link_code_ls:\n",
    "    try:\n",
    "        r = requests.get(symbol_url, headers=headers)\n",
    "        soup_today = BeautifulSoup(r.text, features=\"lxml\")\n",
    "        symbol_code = soup_today.find(name=\"div\", attrs={\"class\": \"board-hq\"}).find(\"span\").text\n",
    "        big_df = pd.DataFrame()\n",
    "        temp_start_year = start_year\n",
    "        \n",
    "        if os.path.exists(f\"./data/{symbol_code}.csv\"):\n",
    "            big_df = pd.read_csv(f\"./data/{symbol_code}.csv\",  encoding=\"utf-8\")\n",
    "            if not big_df.empty:\n",
    "                big_df[\"date\"] = pd.to_datetime(big_df[\"date\"], errors=\"coerce\").dt.date\n",
    "                if big_df[\"date\"].iloc[-1] >= recent_trade_date:\n",
    "                    bk_data_ls.append((name, symbol_code, big_df))\n",
    "                    code_ls.append(symbol_code)\n",
    "                    continue\n",
    "                if big_df['date'].iloc[-1].year > int(temp_start_year):\n",
    "                    temp_start_year = str(big_df[\"date\"].iloc[-1].year)\n",
    "                    idx = big_df.index[big_df['date'].dt.year == int(temp_start_year)].tolist()\n",
    "                    first_idx = idx[0] if idx else None\n",
    "                    if first_idx is not None:\n",
    "                        big_df = big_df.iloc[:first_idx] \n",
    "                elif (big_df['date'].iloc[-1] == lastday):\n",
    "                    dd_texts = [dd.text for dd in soup_today.find_all(\"dd\")] \n",
    "                    close = soup_today.select_one('span.board-xj.arr-rise,  span.board-xj.arr-fall').text\n",
    "                    open, low, high, volume, amount = dd_texts[0], dd_texts[2], dd_texts[3], dd_texts[4], dd_texts[9]\n",
    "                    new_list = [\n",
    "                        recent_trade_date,\n",
    "                        float(symbol_code),\n",
    "                        float(open),\n",
    "                        float(high),\n",
    "                        float(low),\n",
    "                        float(close),\n",
    "                        float(volume.replace(\",\", \"\")),\n",
    "                        float(amount.replace(\",\", \"\")),\n",
    "                    ]\n",
    "                    new_row = pd.DataFrame([new_list], columns=big_df.columns) \n",
    "                    big_df = pd.concat([big_df,  new_row], ignore_index=True)\n",
    "                    bk_data_ls.append((name, symbol_code, big_df))\n",
    "                    code_ls.append(symbol_code)\n",
    "                    big_df.to_csv(f\"./data/{symbol_code}.csv\", index=False, encoding='utf-8-sig')\n",
    "                    continue\n",
    "\n",
    "        tqdm = get_tqdm()\n",
    "        for year in tqdm(range(int(temp_start_year), current_year + 1), leave=False):\n",
    "            url = f\"http://d.10jqka.com.cn/v4/line/bk_{symbol_code}/01/{year}.js\"\n",
    "            r = requests.get(url, headers=headers_bk)\n",
    "            data_text = r.text\n",
    "            try:\n",
    "                demjson.decode(data_text[data_text.find(\"{\") : -1])\n",
    "                temp_df = demjson.decode(data_text[data_text.find(\"{\") : -1])\n",
    "                temp_df = pd.DataFrame(temp_df[\"data\"].split(\";\"))\n",
    "                temp_df = temp_df.iloc[:, 0].str.split(\",\", expand=True)\n",
    "                big_df = pd.concat(objs=[big_df, temp_df], ignore_index=True)\n",
    "            except Exception as e:  # noqa: E722\n",
    "                print(e, \"decode fail:\", url, \"text as:\", data_text)\n",
    "                break\n",
    "        big_df = big_df.iloc[:,  :7]\n",
    "        big_df.columns  = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"amount\"]\n",
    "        big_df.insert(1,  'code', symbol_code)\n",
    "        big_df[\"date\"] = pd.to_datetime(big_df[\"date\"], errors=\"coerce\").dt.date\n",
    "        big_df[\"code\"] = pd.to_numeric(big_df[\"code\"], errors=\"coerce\")\n",
    "        big_df[\"open\"] = pd.to_numeric(big_df[\"open\"], errors=\"coerce\")\n",
    "        big_df[\"high\"] = pd.to_numeric(big_df[\"high\"], errors=\"coerce\")\n",
    "        big_df[\"low\"] = pd.to_numeric(big_df[\"low\"], errors=\"coerce\")\n",
    "        big_df[\"close\"] = pd.to_numeric(big_df[\"close\"], errors=\"coerce\")\n",
    "        big_df[\"volume\"] = pd.to_numeric(big_df[\"volume\"], errors=\"coerce\")\n",
    "        big_df[\"amount\"] = pd.to_numeric(big_df[\"amount\"], errors=\"coerce\")\n",
    "        \n",
    "\n",
    "        if big_df[\"date\"].iloc[-1] != recent_trade_date:\n",
    "            dd_texts = [dd.text for dd in soup_today.find_all(\"dd\")] \n",
    "            close = soup_today.select_one('span.board-xj.arr-rise,  span.board-xj.arr-fall').text\n",
    "            open, low, high, volume, amount = dd_texts[0], dd_texts[2], dd_texts[3], dd_texts[4], dd_texts[9]\n",
    "            new_list = [\n",
    "                recent_trade_date,\n",
    "                float(symbol_code),\n",
    "                float(open),\n",
    "                float(high),\n",
    "                float(low),\n",
    "                float(close),\n",
    "                float(volume.replace(\",\", \"\")),\n",
    "                float(amount.replace(\",\", \"\")),\n",
    "            ]\n",
    "            new_row = pd.DataFrame([new_list], columns=big_df.columns) \n",
    "            big_df = pd.concat([big_df,  new_row], ignore_index=True)\n",
    "        bk_data_ls.append((name, symbol_code, big_df))\n",
    "        code_ls.append(symbol_code)\n",
    "        big_df.to_csv(f\"./data/{symbol_code}.csv\", index=False, encoding='utf-8-sig')\n",
    "    except Exception as e:\n",
    "        print(e, \" wrong data:\", f\"https://d.10jqka.com.cn/v4/line/bk_{symbol_code}/01/{year}.js\", symbol_url, big_df.head())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802ac574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿尔茨海默概念\n",
      "886056\n",
      "           date      code      open      high       low     close  \\\n",
      "168  2025-09-10  886056.0  1325.734  1339.842  1319.055  1327.358   \n",
      "169  2025-09-11  886056.0  1315.080  1340.425  1287.400  1340.425   \n",
      "170  2025-09-12  886056.0  1342.274  1359.268  1337.076  1350.272   \n",
      "171  2025-09-12  886056.0  1342.270  1359.270  1337.080  1350.270   \n",
      "172  2025-09-15  886056.0  1349.380  1359.670  1346.040  1355.560   \n",
      "\n",
      "           volume        amount  \n",
      "168  5.103168e+08  9.774379e+09  \n",
      "169  7.672469e+08  1.480606e+10  \n",
      "170  7.405859e+08  1.411970e+10  \n",
      "171  7.405900e+02  1.412000e+02  \n",
      "172  6.558900e+02  1.272300e+02  \n"
     ]
    }
   ],
   "source": [
    "print(bk_data_ls[0][0])\n",
    "print(bk_data_ls[0][1])\n",
    "print(bk_data_ls[0][2].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94708784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Scalers(scalers, filepath):\n",
    "    \"\"\"保存所有股票的Scaler到文件\"\"\"\n",
    "    joblib.dump(scalers, filepath)\n",
    "def load_Scalers(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        return joblib.load(filepath)\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "badcc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_scaler_path = \"./model/code_scaler.save\"\n",
    "if os.path.exists(code_scaler_path):\n",
    "    code_scaler = joblib.load(code_scaler_path)\n",
    "    if len(code_ls) != len(bk_data_ls):\n",
    "        code_df = pd.DataFrame(code_ls, columns=['code'])\n",
    "        code_scaler.partial_fit(code_df)\n",
    "else:\n",
    "    code_df = pd.DataFrame(code_ls, columns=['code'])\n",
    "    code_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    code_scaler.fit(code_df)\n",
    "    save_Scalers(code_scaler, code_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba57affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, bk_data_ls, seq_length=16, forecast_gap=2, scaler_filepath='./model/scalers.sav'):\n",
    "        \"\"\"\n",
    "        bk_data_ls: List of tuples (code, data), where:\n",
    "            - code: 股票代码（str）\n",
    "            - data: DataFrame，列顺序为 [open, high, low, close, volume, amount]\n",
    "        seq_length_short: 短期序列长度（可选）\n",
    "        seq_length: 长期依赖序列长度（用于输入）\n",
    "        forecast_gap: 预测几天后的收盘价，如后天 = 2\n",
    "        \"\"\"\n",
    "        self.seq_length  = seq_length\n",
    "        self.forecast_gap  = forecast_gap\n",
    "        self.scalers  = load_Scalers(scaler_filepath) # 存储每个股票的归一化器\n",
    " \n",
    "        all_X = []\n",
    "        all_y = []\n",
    "\n",
    "        for _name, code, data in bk_data_ls:\n",
    "            data = data[[\"open\", \"high\", \"low\", \"close\", \"volume\", \"amount\"]]\n",
    "            if code in self.scalers:\n",
    "                scaler = self.scalers[code]\n",
    "                scaled_data = scaler.partial_fit(data).transform(data) \n",
    "                self.scalers[code]  = scaler  # 保存 scaler 供后续使用 \n",
    "            else:\n",
    "                scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                scaled_data = scaler.fit_transform(data) \n",
    "                self.scalers[code]  = scaler  # 保存 scaler 供后续使用 \n",
    "    \n",
    "            # 构造样本\n",
    "            X, y = [], []\n",
    "            max_index = len(data) - seq_length - forecast_gap\n",
    "            for i in range(max_index):\n",
    "                # 输入序列：seq_length 天的特征\n",
    "                seq_features = scaled_data[i:i + seq_length]\n",
    "                code_feature = code_scaler.transform(data[['code']].iloc[i:i + seq_length])\n",
    "                seq_features = np.insert(seq_features,  1, code_feature, axis=1) \n",
    "                # 标签：forecast_gap 天后的收盘价（第3列）\n",
    "                target_idx = i + seq_length + forecast_gap - 1\n",
    "                target_close = scaled_data[target_idx, 3]\n",
    " \n",
    "                X.append(seq_features) \n",
    "                y.append(target_close) \n",
    " \n",
    "            # 转换为 numpy 并保存 \n",
    "            all_X.extend(X) \n",
    "            all_y.extend(y) \n",
    " \n",
    "        # 统一转换为 tensor\n",
    "        self.X = torch.tensor(np.array(all_X),  dtype=torch.float16) \n",
    "        self.y = torch.tensor(np.array(all_y),  dtype=torch.float16).view(-1,  1)\n",
    "        save_Scalers(self.scalers, scaler_filepath)  # 保存所有股票的Scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    " \n",
    "    def get_scaler(self, code):\n",
    "        return self.scalers.get(code,  None)\n",
    "    def inverse_transform(self, code, scaled_values):\n",
    "        \"\"\"逆归一化\"\"\"\n",
    "        scaler = self.get_scaler(code)\n",
    "        if scaler:\n",
    "            dummy = np.zeros((len(scaled_values),  self.X.shape[2]))\n",
    "            dummy[:, 3] = scaled_values  # 将预测值放入close列\n",
    "            return scaler.inverse_transform(dummy)[:, 3]\n",
    "        else:\n",
    "            raise ValueError(f\"No scaler found for code: {code}\")\n",
    "\n",
    "    \n",
    "\n",
    "class MultiScaleAttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 64, num_layers_long = 3, num_layers_short = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_long  = nn.LSTM(input_size, hidden_size, num_layers=num_layers_long, batch_first=True, dropout=0.2 if num_layers_long > 1 else 0)\n",
    "        self.lstm_short  = nn.LSTM(input_size, hidden_size, num_layers=num_layers_short, batch_first=True, dropout=0.2 if num_layers_short > 1 else 0)\n",
    "        self.attn  = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True, )  # 4头注意力 \n",
    "        \n",
    "        self.regressor  = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x1, seq_length_short=7):\n",
    "        out1, _ = self.lstm_long(x1) \n",
    "        out2, _ = self.lstm_short(x1[:, -seq_length_short:, :]) # lstm_out: [batch, seq_len, hidden]\n",
    "        lstm_out = torch.cat((out1,  out2), dim=1)\n",
    "        attn_out, _ = self.attn(lstm_out,  lstm_out, lstm_out, need_weights=False) \n",
    "        out = self.regressor(attn_out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63a9e7ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['code'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 创建数据集 \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mStockDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbk_data_ls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m      4\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split( \n\u001b[0;32m      5\u001b[0m     dataset, [train_size, \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size]\n\u001b[0;32m      6\u001b[0m )\n",
      "Cell \u001b[1;32mIn[24], line 35\u001b[0m, in \u001b[0;36mStockDataset.__init__\u001b[1;34m(self, bk_data_ls, seq_length, forecast_gap, scaler_filepath)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_index):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# 输入序列：seq_length 天的特征\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     seq_features \u001b[38;5;241m=\u001b[39m scaled_data[i:i \u001b[38;5;241m+\u001b[39m seq_length]\n\u001b[1;32m---> 35\u001b[0m     code_feature \u001b[38;5;241m=\u001b[39m code_scaler\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[i:i \u001b[38;5;241m+\u001b[39m seq_length])\n\u001b[0;32m     36\u001b[0m     seq_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(seq_features,  \u001b[38;5;241m1\u001b[39m, code_feature, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# 标签：forecast_gap 天后的收盘价（第3列）\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6263\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['code'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# 创建数据集 \n",
    "dataset = StockDataset(bk_data_ls, seq_length=16, forecast_gap=2)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_set, test_set = torch.utils.data.random_split( \n",
    "    dataset, [train_size, len(dataset) - train_size]\n",
    ")\n",
    "\n",
    "# 数据加载器 \n",
    "train_loader = DataLoader(train_set, batch_size=100000, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=100000)\n",
    "\n",
    "# 模型初始化\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 加载模型（需先实例化结构）\n",
    "model = MultiScaleAttentionLSTM(\n",
    "    input_size=7,  # 7个特征 \n",
    "    hidden_size=64,\n",
    "    num_layers_long = 3, \n",
    "    num_layers_short = 2\n",
    ").to(device)\n",
    "\n",
    "if os.path.exists('./model/model.pth'):\n",
    "    model.load_state_dict(torch.load('./model/model.pth')) \n",
    "\n",
    "# 训练参数 \n",
    "criterion = nn.MSELoss()\n",
    "scaler = amp.GradScaler(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "    optimizer, \n",
    "    mode='min',       # 监控验证损失\n",
    "    factor=0.5,       # 学习率衰减系数 \n",
    "    patience=5,       # 容忍5个epoch无改善\n",
    ")\n",
    "\n",
    "# ===== 4. 训练循环 =====\n",
    "for epoch in range(100):\n",
    "    model.train() \n",
    "    train_loss = 0 \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device),  y_batch.to(device) \n",
    "        \n",
    "        with amp.autocast(device_type=device): \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        # 反向传播（自动处理精度）\n",
    "        scaler.scale(loss).backward()   # 缩放梯度\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)          # 更新参数 \n",
    "        scaler.update()                 # 调整缩放因子 \n",
    "        optimizer.zero_grad() \n",
    "    print(f\"Epoch {epoch+1}: Current LR = {scheduler.get_last_lr()[0]}\") \n",
    "    # 验证\n",
    "    model.eval() \n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device),  y_test.to(device) \n",
    "            preds = model(X_test)\n",
    "            test_loss += criterion(preds, y_test).item()\n",
    "    \n",
    "    scheduler.step(test_loss) \n",
    "    print(f'Epoch {epoch} | Train Loss: {train_loss/len(train_loader):.6f} | Test Loss: {test_loss/len(test_loader):.6f}')\n",
    "\n",
    "torch.save(model.state_dict(),  './model/model.pth') \n",
    "\n",
    "# ===== 5. 预测示例 =====\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    for name, code, data in bk_data_ls:\n",
    "        scaler = dataset.get_scaler(code)\n",
    "        max_index = len(data) - dataset.seq_length\n",
    "        seq_features = scaler.transform(data[[\"open\", \"high\", \"low\", \"close\", \"volume\", \"amount\"]].iloc[max_index:])\n",
    "        code_feature = code_scaler.transform(data[['code']].iloc[max_index:])\n",
    "        seq_features = np.insert(seq_features,  1, code_feature, axis=1)\n",
    "        X = torch.tensor(seq_features,  dtype=torch.float16).to(device)\n",
    "        X=X.unsqueeze(0)  # 添加批次维度\n",
    "\n",
    "        prediction = model(X)\n",
    "        scaled_pred = prediction.cpu().numpy() \n",
    "        \n",
    "        # 逆归一化收盘价 \n",
    "        dummy = np.zeros((1,  6))\n",
    "        dummy[:, 3] = scaled_pred  # 将预测值放入close列\n",
    "        real_pred = scaler.inverse_transform(dummy)[0,  3]\n",
    "        if real_pred > data.iloc[-1,  3]:\n",
    "            print(\"high\")\n",
    "        else:\n",
    "            print(\"low\")\n",
    "        print(f'stock name: {name}, stock code: {code}, 今天收盘价：{data.iloc[-1,  3]}, 预测的后天收盘价: {real_pred:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a6d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c8eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b84f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ed238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
