{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a97e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from akshare.utils import demjson\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from akshare.tool.trade_date_hist import tool_trade_date_hist_sina\n",
    "\n",
    "# ===== 1. 数据预处理 ===== \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch  import amp\n",
    "from torch.utils.data  import Dataset, DataLoader\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "# import py_mini_racer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b676539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近交易日：2025-09-16\n",
      "上一个交易日：2025-09-15\n"
     ]
    }
   ],
   "source": [
    "# def is_trading_day():\n",
    "#     today = datetime.now().strftime('%Y-%m-%d') \n",
    "#     # 获取交易日历（新浪财经）\n",
    "#     trade_date_df = tool_trade_date_hist_sina() \n",
    "#     # 判断今天是否在交易日历中\n",
    "#     return today in trade_date_df['trade_date'].values\n",
    "\n",
    "# if is_trading_day():\n",
    "#     print(\"今天是交易日\")\n",
    "# else:\n",
    "#     print(\"今天不是交易日\")\n",
    "\n",
    "# is_trading_day = is_trading_day()\n",
    "# today = datetime.now().date()\n",
    "trade_date_df = tool_trade_date_hist_sina() \n",
    " \n",
    "# 确保日期列是字符串格式\n",
    "trade_date_list = trade_date_df['trade_date'].astype(str).tolist()\n",
    " \n",
    "# 获取当前日期\n",
    "today = datetime.now().date() \n",
    "lastday = today - timedelta(days=1)\n",
    "# 找到最近交易日\n",
    "recent_trade_date = today \n",
    "while str(recent_trade_date) not in trade_date_list:\n",
    "    recent_trade_date -= timedelta(days=1)\n",
    "    lastday -= timedelta(days=1)\n",
    "while str(lastday) not in trade_date_list:\n",
    "    lastday -= timedelta(days=1)\n",
    " \n",
    "print(f\"最近交易日：{recent_trade_date}\")\n",
    "print(f\"上一个交易日：{lastday}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de3500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = \"2025\"\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# js_code = py_mini_racer.MiniRacer()\n",
    "# with open('ths.js', 'r', encoding=\"utf-8\") as f:\n",
    "#     js_content = f.read()\n",
    "# js_code.eval(js_content)\n",
    "# v_code = js_code.call(\"v\")\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "        # \"Cookie\": f\"v={v_code}\",\n",
    "    }\n",
    "headers_bk = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "            \"Referer\": \"https://q.10jqka.com.cn\",\n",
    "            \"Host\": \"d.10jqka.com.cn\",\n",
    "            # \"Cookie\": f\"v={v_code}\",\n",
    "        }\n",
    "\n",
    "gn_link_code_ls = []\n",
    "thshy_link_code_ls = []\n",
    "bk_data_ls = []\n",
    "code_ls = []\n",
    "\n",
    "for suffix in [\"gn/\",\"thshy/\"]:\n",
    "    url = f\"https://q.10jqka.com.cn/{suffix}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, features=\"lxml\")\n",
    "    divs = soup.find_all(name=\"div\", attrs={\"class\": \"cate_items\"})\n",
    "    for div in divs:\n",
    "        # 在每个div中查找所有的a标签\n",
    "        links = div.find_all('a') \n",
    "        for link in links:\n",
    "            # 获取href属性\n",
    "            href = link.get('href') \n",
    "            # 获取文本内容\n",
    "            text = link.text\n",
    "            if suffix == \"thshy/\":\n",
    "                parts = href.rstrip('/').split('/')\n",
    "                code = parts[-1]\n",
    "                thshy_link_code_ls.append((code,  text))\n",
    "            else:\n",
    "                gn_link_code_ls.append((href,  text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157315ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    }
   ],
   "source": [
    "print(len(gn_link_code_ls))\n",
    "count = 0\n",
    "for symbol_url, name in gn_link_code_ls:\n",
    "    try:\n",
    "        r = requests.get(symbol_url, headers=headers)\n",
    "        soup_today = BeautifulSoup(r.text, features=\"lxml\")\n",
    "        symbol_code = soup_today.find(name=\"div\", attrs={\"class\": \"board-hq\"}).find(\"span\").text\n",
    "        big_df = pd.DataFrame()\n",
    "        temp_start_year = start_year\n",
    "        \n",
    "        if os.path.exists(f\"./data/{symbol_code}.csv\"):\n",
    "            big_df = pd.read_csv(f\"./data/{symbol_code}.csv\",  encoding=\"utf-8\", header=None)\n",
    "            if not big_df.empty:\n",
    "                big_df.iloc[:, 0] = pd.to_datetime(big_df.iloc[:, 0], errors=\"coerce\").dt.date\n",
    "                if big_df.iloc[-1, 0] >= recent_trade_date:\n",
    "                    bk_data_ls.append((name, symbol_code, big_df))\n",
    "                    code_ls.append(symbol_code)\n",
    "                    continue\n",
    "                elif big_df.iloc[-1, 0].year >= int(temp_start_year):\n",
    "                    temp_start_year = str(big_df.iloc[-1, 0].year)\n",
    "                    year_mask = big_df.iloc[:,0].apply(lambda x: x.year)  ==  int(temp_start_year) \n",
    "                    if year_mask.any():\n",
    "                        first_index = year_mask.idxmax()\n",
    "                    else:\n",
    "                        first_index = None\n",
    "                    if first_index is not None:\n",
    "                        big_df = big_df.iloc[:first_index]\n",
    "                elif (big_df.iloc[-1, 0] == lastday):\n",
    "                    dd_texts = [dd.text for dd in soup_today.find_all(\"dd\")] \n",
    "                    close = soup_today.select_one('span.board-xj.arr-rise,  span.board-xj.arr-fall').text\n",
    "                    open, low, high, volume, amount = dd_texts[0], dd_texts[2], dd_texts[3], dd_texts[4], dd_texts[9]\n",
    "                    new_list = [\n",
    "                        recent_trade_date,\n",
    "                        float(open),\n",
    "                        float(high),\n",
    "                        float(low),\n",
    "                        float(close),\n",
    "                        float(volume.replace(\",\", \"\")),\n",
    "                        float(amount.replace(\",\", \"\")),\n",
    "                    ]\n",
    "                    new_row = pd.DataFrame([new_list], columns=big_df.columns) \n",
    "                    big_df = pd.concat([big_df,  new_row], ignore_index=True)\n",
    "                    bk_data_ls.append((name, symbol_code, big_df))\n",
    "                    code_ls.append(symbol_code)\n",
    "                    big_df.to_csv(f\"./data/{symbol_code}.csv\", index=False, encoding='utf-8-sig')\n",
    "                    continue\n",
    "        \n",
    "        headers_d = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "            \"Referer\": symbol_url,\n",
    "            \"Host\": \"d.10jqka.com.cn\"\n",
    "        }\n",
    "        time.sleep(1)\n",
    "        for year in tqdm(range(int(temp_start_year), current_year + 1), leave=False):\n",
    "            url = f\"http://d.10jqka.com.cn/v4/line/bk_{symbol_code}/01/{year}.js\"\n",
    "            r = requests.get(url, headers=headers_d)\n",
    "            data_text = r.text\n",
    "            if r.status_code != 200:\n",
    "                print(\"not 200!\", r.status_code, symbol_url, url)\n",
    "                continue\n",
    "            try:\n",
    "                temp_df = demjson.decode(data_text[data_text.find(\"{\") : -1])\n",
    "                temp_df = pd.DataFrame(temp_df[\"data\"].split(\";\"))\n",
    "                temp_df = temp_df.iloc[:, 0].str.split(\",\", expand=True).iloc[:,  :7]\n",
    "                big_df = pd.concat(objs=[big_df, temp_df], ignore_index=True)\n",
    "            except Exception as e:  # noqa: E722\n",
    "                print(e, \"decode fail:\",count,\"  \", url, \"text as:\", data_text)\n",
    "                break\n",
    "        big_df.columns  = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"amount\"]\n",
    "        big_df[\"date\"] = pd.to_datetime(big_df[\"date\"], errors=\"coerce\").dt.date\n",
    "        big_df[\"open\"] = pd.to_numeric(big_df[\"open\"], errors=\"coerce\")\n",
    "        big_df[\"high\"] = pd.to_numeric(big_df[\"high\"], errors=\"coerce\")\n",
    "        big_df[\"low\"] = pd.to_numeric(big_df[\"low\"], errors=\"coerce\")\n",
    "        big_df[\"close\"] = pd.to_numeric(big_df[\"close\"], errors=\"coerce\")\n",
    "        big_df[\"volume\"] = pd.to_numeric(big_df[\"volume\"], errors=\"coerce\")\n",
    "        big_df[\"amount\"] = pd.to_numeric(big_df[\"amount\"], errors=\"coerce\")\n",
    "        \n",
    "        if big_df.iloc[-1].isna().any(): \n",
    "            big_df = big_df.iloc[:-1]\n",
    "\n",
    "        if big_df[\"date\"].iloc[-1] == lastday:\n",
    "            dd_texts = [dd.text for dd in soup_today.find_all(\"dd\")] \n",
    "            close = soup_today.select_one('span.board-xj.arr-rise,  span.board-xj.arr-fall').text\n",
    "            open, low, high, volume, amount = dd_texts[0], dd_texts[2], dd_texts[3], dd_texts[4], dd_texts[9]\n",
    "            new_list = [\n",
    "                recent_trade_date,\n",
    "                float(open),\n",
    "                float(high),\n",
    "                float(low),\n",
    "                float(close),\n",
    "                float(volume.replace(\",\", \"\")),\n",
    "                float(amount.replace(\",\", \"\")),\n",
    "            ]\n",
    "            new_row = pd.DataFrame([new_list], columns=big_df.columns) \n",
    "            big_df = pd.concat([big_df,  new_row], ignore_index=True)\n",
    "        bk_data_ls.append((name, symbol_code, big_df))\n",
    "        code_ls.append(symbol_code)\n",
    "        big_df.to_csv(f\"./data/{symbol_code}.csv\", index=False, encoding='utf-8-sig', header=None)\n",
    "        count+=1\n",
    "    except Exception as e:\n",
    "        print(e, \" wrong data:\",count,\"  \", f\"https://d.10jqka.com.cn/v4/line/bk_{symbol_code}/01/{year}.js\", symbol_url, big_df.tail())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3a45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # http://q.10jqka.com.cn/thshy/detail/code/881138/\n",
    "# headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "#         \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "#         # \"Cookie\": f\"v={v_code}\",\n",
    "#     }\n",
    "# headers_bk = {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "#             \"Chrome/89.0.4389.90 Safari/537.36\",\n",
    "#             \"Referer\": \"https://q.10jqka.com.cn\",\n",
    "#             \"Host\": \"d.10jqka.com.cn\",\n",
    "#             # \"Cookie\": f\"v={v_code}\",\n",
    "#         }\n",
    "# url =  \"https://d.10jqka.com.cn/v4/line/bk_885829/01/2025.js\"\n",
    "# symbol_url = \"https://q.10jqka.com.cn/gn/detail/code/301252/\"\n",
    "# print(symbol_url)\n",
    "# r = requests.get(symbol_url, headers=headers)\n",
    "# print(r.status_code)\n",
    "# for name, symbol_code, big_df in bk_data_ls:\n",
    "#     print(symbol_code, big_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b5bc6658c64fb7a0492c4f9ad2ef2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0956e89add534ca98de7b81aaea7aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742427517b0a43eab7679f3ca6ec6284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d6ab8803154a34820db4bbaf264b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32f6c2e50f54c2897bdbcabedba7494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e0bc1382694f71bfb6ed3999e35e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dd44cb6e1d41ef845c539614e22726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153514407d76410c88883fc267559584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(len(thshy_link_code_ls))\n",
    "\n",
    "for symbol_code, name in thshy_link_code_ls:\n",
    "    big_df = pd.DataFrame()\n",
    "    if os.path.exists(f\"./data/{symbol_code}.csv\"):\n",
    "        big_df = pd.read_csv(f\"./data/{symbol_code}.csv\",  encoding=\"utf-8\", header=None)\n",
    "        if not big_df.empty:\n",
    "            big_df.iloc[:, 0] = pd.to_datetime(big_df.iloc[:, 0], errors=\"coerce\").dt.date\n",
    "            if big_df.iloc[-1, 0] >= recent_trade_date:\n",
    "                bk_data_ls.append((name, symbol_code, big_df))\n",
    "                code_ls.append(symbol_code)\n",
    "                continue\n",
    "            elif big_df.iloc[-1, 0].year >= int(temp_start_year):\n",
    "                temp_start_year = str(big_df.iloc[-1, 0].year)\n",
    "                year_mask = big_df.iloc[:,0].apply(lambda x: x.year)  ==  int(temp_start_year) \n",
    "                if year_mask.any():\n",
    "                    first_index = year_mask.idxmax()\n",
    "                else:\n",
    "                    first_index = None\n",
    "                if first_index is not None:\n",
    "                    big_df = big_df.iloc[:first_index]\n",
    "\n",
    "    try:\n",
    "        for year in tqdm(range(int(temp_start_year), current_year + 1), leave=False):\n",
    "            url = f\"http://d.10jqka.com.cn/v4/line/bk_{symbol_code}/01/{year}.js\"\n",
    "            r = requests.get(url, headers=headers_bk)\n",
    "            data_text = r.text\n",
    "            try:\n",
    "                temp_df = demjson.decode(data_text[data_text.find(\"{\") : -1])\n",
    "                temp_df = pd.DataFrame(temp_df[\"data\"].split(\";\"))\n",
    "                temp_df = temp_df.iloc[:, 0].str.split(\",\", expand=True)\n",
    "                big_df = pd.concat(objs=[big_df, temp_df], ignore_index=True)\n",
    "            except Exception as e:  # noqa: E722\n",
    "                print(e, \"decode fail:\", url, \"text as:\", data_text)\n",
    "                break\n",
    "        big_df = big_df.iloc[:,  :7]\n",
    "        big_df.columns  = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"amount\"]\n",
    "        big_df[\"date\"] = pd.to_datetime(big_df[\"date\"], errors=\"coerce\").dt.date\n",
    "        big_df[\"open\"] = pd.to_numeric(big_df[\"open\"], errors=\"coerce\")\n",
    "        big_df[\"high\"] = pd.to_numeric(big_df[\"high\"], errors=\"coerce\")\n",
    "        big_df[\"low\"] = pd.to_numeric(big_df[\"low\"], errors=\"coerce\")\n",
    "        big_df[\"close\"] = pd.to_numeric(big_df[\"close\"], errors=\"coerce\")\n",
    "        big_df[\"volume\"] = pd.to_numeric(big_df[\"volume\"], errors=\"coerce\")\n",
    "        big_df[\"amount\"] = pd.to_numeric(big_df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "        if big_df.iloc[-1].isna().any(): \n",
    "            big_df = big_df.iloc[:-1]\n",
    "\n",
    "        if big_df[\"date\"].iloc[-1] == lastday:\n",
    "            symbol_url = f\"https://q.10jqka.com.cn/thshy/detail/code/{symbol_code}/\"\n",
    "            r = requests.get(symbol_url, headers=headers)\n",
    "            soup_today = BeautifulSoup(r.text, features=\"lxml\")\n",
    "            dd_texts = [dd.text for dd in soup_today.find_all(\"dd\")] \n",
    "            close = soup_today.select_one('span.board-xj.arr-rise,  span.board-xj.arr-fall').text\n",
    "            open, low, high, volume, amount = dd_texts[0], dd_texts[2], dd_texts[3], dd_texts[4], dd_texts[9]\n",
    "            new_list = [\n",
    "                recent_trade_date,\n",
    "                float(open),\n",
    "                float(high),\n",
    "                float(low),\n",
    "                float(close),\n",
    "                float(volume.replace(\",\", \"\")),\n",
    "                float(amount.replace(\",\", \"\")),\n",
    "            ]\n",
    "            new_row = pd.DataFrame([new_list], columns=big_df.columns) \n",
    "            big_df = pd.concat([big_df,  new_row], ignore_index=True)\n",
    "       \n",
    "        bk_data_ls.append((name, symbol_code, big_df))\n",
    "        code_ls.append(symbol_code)\n",
    "        big_df.to_csv(f\"./data/{symbol_code}.csv\", index=False, encoding='utf-8-sig', header=None)\n",
    "    except Exception as e:\n",
    "        print(e, \" wrong data:\", f\"https://d.10jqka.com.cn/v4/line/bk_{symbol_code}/01/{year}.js\", symbol_url, big_df.tail())\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802ac574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿尔茨海默概念\n",
      "886056\n",
      "            0        1        2        3        4          5             6\n",
      "0  2025-01-02  989.548  999.124  968.150  974.704  512509970  6.680677e+09\n",
      "1  2025-01-03  974.714  986.675  962.398  965.056  475802500  6.222582e+09\n",
      "2  2025-01-06  971.721  987.276  960.495  979.601  525790230  6.659436e+09\n",
      "3  2025-01-07  979.367  979.367  951.360  969.033  966740090  1.109609e+10\n",
      "4  2025-01-08  967.908  974.077  945.650  965.447  771668230  9.432990e+09\n",
      "              0         1         2         3         4          5  \\\n",
      "168  2025-09-10  1325.734  1339.842  1319.055  1327.358  510316830   \n",
      "169  2025-09-11  1315.080  1340.425  1287.400  1340.425  767246910   \n",
      "170  2025-09-12  1342.274  1359.268  1337.076  1350.272  740585930   \n",
      "171  2025-09-15  1349.381  1359.669  1346.040  1355.557  655885750   \n",
      "172  2025-09-16  1354.393  1357.349  1349.074  1349.074  235921160   \n",
      "\n",
      "                6  \n",
      "168  9.774379e+09  \n",
      "169  1.480606e+10  \n",
      "170  1.411970e+10  \n",
      "171  1.272261e+10  \n",
      "172  4.108447e+09  \n"
     ]
    }
   ],
   "source": [
    "print(bk_data_ls[0][0])\n",
    "print(bk_data_ls[0][1])\n",
    "print(bk_data_ls[0][2].head())\n",
    "print(bk_data_ls[0][2].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94708784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Scalers(scalers, filepath):\n",
    "    \"\"\"保存所有股票的Scaler到文件\"\"\"\n",
    "    joblib.dump(scalers, filepath)\n",
    "def load_Scalers(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        return joblib.load(filepath)\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "badcc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_scaler_path = \"./model/code_scaler.save\"\n",
    "if os.path.exists(code_scaler_path):\n",
    "    code_scaler = joblib.load(code_scaler_path)\n",
    "    if len(code_ls) != len(bk_data_ls):\n",
    "        code_df = pd.DataFrame(code_ls, columns=['code'])\n",
    "        code_scaler.partial_fit(code_df)\n",
    "else:\n",
    "    code_df = pd.DataFrame(code_ls, columns=['code'])\n",
    "    code_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    code_scaler.fit(code_df)\n",
    "    save_Scalers(code_scaler, code_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba57affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, bk_data_ls, seq_length=16, forecast_gap=2, scaler_filepath='./model/scalers.sav'):\n",
    "        \"\"\"\n",
    "        bk_data_ls: List of tuples (code, data), where:\n",
    "            - code: 股票代码（str）\n",
    "            - data: DataFrame，列顺序为 [open, high, low, close, volume, amount]\n",
    "        seq_length_short: 短期序列长度（可选）\n",
    "        seq_length: 长期依赖序列长度（用于输入）\n",
    "        forecast_gap: 预测几天后的收盘价，如后天 = 2\n",
    "        \"\"\"\n",
    "        self.seq_length  = seq_length\n",
    "        self.forecast_gap  = forecast_gap\n",
    "        self.scalers  = load_Scalers(scaler_filepath) # 存储每个股票的归一化器\n",
    " \n",
    "        all_X = []\n",
    "        all_y = []\n",
    "\n",
    "        for _name, code, data in bk_data_ls:\n",
    "            data = data.iloc[:,1:]\n",
    "            if code in self.scalers:\n",
    "                scaler = self.scalers[code]\n",
    "                scaled_data = scaler.partial_fit(data).transform(data) \n",
    "                self.scalers[code]  = scaler  # 保存 scaler 供后续使用 \n",
    "            else:\n",
    "                scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                scaled_data = scaler.fit_transform(data) \n",
    "                self.scalers[code]  = scaler  # 保存 scaler 供后续使用 \n",
    "    \n",
    "            # 构造样本\n",
    "            X, y = [], []\n",
    "            code_feature = code_scaler.transform(pd.DataFrame({\"code\": [code]}))\n",
    "            scaled_data = np.insert(scaled_data,  1, code_feature, axis=1) \n",
    "            max_index = len(data) - seq_length - forecast_gap\n",
    "            for i in range(max_index):\n",
    "                # 输入序列：seq_length 天的特征\n",
    "                seq_features = scaled_data[i:i + seq_length]\n",
    "                # 标签：forecast_gap 天后的收盘价（第3列）\n",
    "                target_idx = i + seq_length + forecast_gap - 1\n",
    "                target_close = scaled_data[target_idx, 3]\n",
    " \n",
    "                X.append(seq_features) \n",
    "                y.append(target_close) \n",
    " \n",
    "            # 转换为 numpy 并保存 \n",
    "            all_X.extend(X) \n",
    "            all_y.extend(y) \n",
    " \n",
    "        # 统一转换为 tensor\n",
    "        self.X = torch.tensor(np.array(all_X),  dtype=torch.float32) \n",
    "        self.y = torch.tensor(np.array(all_y),  dtype=torch.float32).view(-1,  1)\n",
    "        save_Scalers(self.scalers, scaler_filepath)  # 保存所有股票的Scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    " \n",
    "    def get_scaler(self, code):\n",
    "        return self.scalers.get(code,  None)\n",
    "    def inverse_transform(self, code, scaled_values):\n",
    "        \"\"\"逆归一化\"\"\"\n",
    "        scaler = self.get_scaler(code)\n",
    "        if scaler:\n",
    "            dummy = np.zeros((len(scaled_values),  self.X.shape[2]))\n",
    "            dummy[:, 3] = scaled_values  # 将预测值放入close列\n",
    "            return scaler.inverse_transform(dummy)[:, 3]\n",
    "        else:\n",
    "            raise ValueError(f\"No scaler found for code: {code}\")\n",
    "\n",
    "    \n",
    "\n",
    "class MultiScaleAttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 64, num_layers_long = 3, num_layers_short = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_long  = nn.LSTM(input_size, hidden_size, num_layers=num_layers_long, batch_first=True, dropout=0.2 if num_layers_long > 1 else 0)\n",
    "        self.lstm_short  = nn.LSTM(input_size, hidden_size, num_layers=num_layers_short, batch_first=True, dropout=0.2 if num_layers_short > 1 else 0)\n",
    "        self.attn  = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True, )  # 4头注意力 \n",
    "        \n",
    "        self.regressor  = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x1, seq_length_short=7):\n",
    "        out1, _ = self.lstm_long(x1) \n",
    "        out2, _ = self.lstm_short(x1[:, -seq_length_short:, :]) # lstm_out: [batch, seq_len, hidden]\n",
    "        lstm_out = torch.cat((out1,  out2), dim=1)\n",
    "        attn_out, _ = self.attn(lstm_out,  lstm_out, lstm_out, need_weights=False) \n",
    "        out = self.regressor(attn_out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63a9e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Current LR = 0.001\n",
      "Epoch 0 | Train Loss: 0.417091 | Test Loss: 0.416587\n",
      "Epoch 2: Current LR = 0.001\n",
      "Epoch 1 | Train Loss: 0.417096 | Test Loss: 0.397862\n",
      "Epoch 3: Current LR = 0.001\n",
      "Epoch 2 | Train Loss: 0.398383 | Test Loss: 0.380474\n",
      "Epoch 4: Current LR = 0.001\n",
      "Epoch 3 | Train Loss: 0.380979 | Test Loss: 0.363891\n",
      "Epoch 5: Current LR = 0.001\n",
      "Epoch 4 | Train Loss: 0.364427 | Test Loss: 0.347612\n",
      "Epoch 6: Current LR = 0.001\n",
      "Epoch 5 | Train Loss: 0.348147 | Test Loss: 0.331190\n",
      "Epoch 7: Current LR = 0.001\n",
      "Epoch 6 | Train Loss: 0.331719 | Test Loss: 0.314258\n",
      "Epoch 8: Current LR = 0.001\n",
      "Epoch 7 | Train Loss: 0.314800 | Test Loss: 0.296511\n",
      "Epoch 9: Current LR = 0.001\n",
      "Epoch 8 | Train Loss: 0.297071 | Test Loss: 0.277704\n",
      "Epoch 10: Current LR = 0.001\n",
      "Epoch 9 | Train Loss: 0.278278 | Test Loss: 0.257637\n",
      "Epoch 11: Current LR = 0.001\n",
      "Epoch 10 | Train Loss: 0.258207 | Test Loss: 0.236166\n",
      "Epoch 12: Current LR = 0.001\n",
      "Epoch 11 | Train Loss: 0.236761 | Test Loss: 0.213211\n",
      "Epoch 13: Current LR = 0.001\n",
      "Epoch 12 | Train Loss: 0.213808 | Test Loss: 0.188799\n",
      "Epoch 14: Current LR = 0.001\n",
      "Epoch 13 | Train Loss: 0.189389 | Test Loss: 0.163120\n",
      "Epoch 15: Current LR = 0.001\n",
      "Epoch 14 | Train Loss: 0.163735 | Test Loss: 0.136624\n",
      "Epoch 16: Current LR = 0.001\n",
      "Epoch 15 | Train Loss: 0.137278 | Test Loss: 0.110162\n",
      "Epoch 17: Current LR = 0.001\n",
      "Epoch 16 | Train Loss: 0.110817 | Test Loss: 0.085192\n",
      "Epoch 18: Current LR = 0.001\n",
      "Epoch 17 | Train Loss: 0.085854 | Test Loss: 0.064108\n",
      "Epoch 19: Current LR = 0.001\n",
      "Epoch 18 | Train Loss: 0.064745 | Test Loss: 0.050727\n",
      "Epoch 20: Current LR = 0.001\n",
      "Epoch 19 | Train Loss: 0.051353 | Test Loss: 0.050989\n",
      "Epoch 21: Current LR = 0.001\n",
      "Epoch 20 | Train Loss: 0.051537 | Test Loss: 0.062354\n",
      "Epoch 22: Current LR = 0.001\n",
      "Epoch 21 | Train Loss: 0.062818 | Test Loss: 0.074016\n",
      "Epoch 23: Current LR = 0.001\n",
      "Epoch 22 | Train Loss: 0.074426 | Test Loss: 0.079638\n",
      "Epoch 24: Current LR = 0.001\n",
      "Epoch 23 | Train Loss: 0.080009 | Test Loss: 0.077807\n",
      "Epoch 25: Current LR = 0.001\n",
      "Epoch 24 | Train Loss: 0.078209 | Test Loss: 0.070468\n",
      "Epoch 26: Current LR = 0.0005\n",
      "Epoch 25 | Train Loss: 0.070891 | Test Loss: 0.065348\n",
      "Epoch 27: Current LR = 0.0005\n",
      "Epoch 26 | Train Loss: 0.065796 | Test Loss: 0.059683\n",
      "Epoch 28: Current LR = 0.0005\n",
      "Epoch 27 | Train Loss: 0.060171 | Test Loss: 0.054410\n",
      "Epoch 29: Current LR = 0.0005\n",
      "Epoch 28 | Train Loss: 0.054925 | Test Loss: 0.050240\n",
      "Epoch 30: Current LR = 0.0005\n",
      "Epoch 29 | Train Loss: 0.050794 | Test Loss: 0.047648\n",
      "Epoch 31: Current LR = 0.0005\n",
      "Epoch 30 | Train Loss: 0.048223 | Test Loss: 0.046859\n",
      "Epoch 32: Current LR = 0.0005\n",
      "Epoch 31 | Train Loss: 0.047449 | Test Loss: 0.047191\n",
      "Epoch 33: Current LR = 0.0005\n",
      "Epoch 32 | Train Loss: 0.047794 | Test Loss: 0.047690\n",
      "Epoch 34: Current LR = 0.0005\n",
      "Epoch 33 | Train Loss: 0.048306 | Test Loss: 0.047895\n",
      "Epoch 35: Current LR = 0.0005\n",
      "Epoch 34 | Train Loss: 0.048508 | Test Loss: 0.047655\n",
      "Epoch 36: Current LR = 0.0005\n",
      "Epoch 35 | Train Loss: 0.048275 | Test Loss: 0.047002\n",
      "Epoch 37: Current LR = 0.0005\n",
      "Epoch 36 | Train Loss: 0.047620 | Test Loss: 0.046085\n",
      "Epoch 38: Current LR = 0.0005\n",
      "Epoch 37 | Train Loss: 0.046703 | Test Loss: 0.045139\n",
      "Epoch 39: Current LR = 0.0005\n",
      "Epoch 38 | Train Loss: 0.045750 | Test Loss: 0.044471\n",
      "Epoch 40: Current LR = 0.0005\n",
      "Epoch 39 | Train Loss: 0.045075 | Test Loss: 0.044256\n",
      "Epoch 41: Current LR = 0.0005\n",
      "Epoch 40 | Train Loss: 0.044864 | Test Loss: 0.044256\n",
      "Epoch 42: Current LR = 0.0005\n",
      "Epoch 41 | Train Loss: 0.044847 | Test Loss: 0.044100\n",
      "Epoch 43: Current LR = 0.0005\n",
      "Epoch 42 | Train Loss: 0.044698 | Test Loss: 0.043625\n",
      "Epoch 44: Current LR = 0.0005\n",
      "Epoch 43 | Train Loss: 0.044226 | Test Loss: 0.042846\n",
      "Epoch 45: Current LR = 0.0005\n",
      "Epoch 44 | Train Loss: 0.043449 | Test Loss: 0.041917\n",
      "Epoch 46: Current LR = 0.0005\n",
      "Epoch 45 | Train Loss: 0.042531 | Test Loss: 0.041056\n",
      "Epoch 47: Current LR = 0.0005\n",
      "Epoch 46 | Train Loss: 0.041677 | Test Loss: 0.040429\n",
      "Epoch 48: Current LR = 0.0005\n",
      "Epoch 47 | Train Loss: 0.041063 | Test Loss: 0.039834\n",
      "Epoch 49: Current LR = 0.0005\n",
      "Epoch 48 | Train Loss: 0.040484 | Test Loss: 0.039051\n",
      "Epoch 50: Current LR = 0.0005\n",
      "Epoch 49 | Train Loss: 0.039691 | Test Loss: 0.038043\n",
      "Epoch 51: Current LR = 0.0005\n",
      "Epoch 50 | Train Loss: 0.038706 | Test Loss: 0.036914\n",
      "Epoch 52: Current LR = 0.0005\n",
      "Epoch 51 | Train Loss: 0.037557 | Test Loss: 0.035807\n",
      "Epoch 53: Current LR = 0.0005\n",
      "Epoch 52 | Train Loss: 0.036449 | Test Loss: 0.034704\n",
      "Epoch 54: Current LR = 0.0005\n",
      "Epoch 53 | Train Loss: 0.035347 | Test Loss: 0.033484\n",
      "Epoch 55: Current LR = 0.0005\n",
      "Epoch 54 | Train Loss: 0.034139 | Test Loss: 0.032098\n",
      "Epoch 56: Current LR = 0.0005\n",
      "Epoch 55 | Train Loss: 0.032766 | Test Loss: 0.030596\n",
      "Epoch 57: Current LR = 0.0005\n",
      "Epoch 56 | Train Loss: 0.031270 | Test Loss: 0.029064\n",
      "Epoch 58: Current LR = 0.0005\n",
      "Epoch 57 | Train Loss: 0.029763 | Test Loss: 0.027509\n",
      "Epoch 59: Current LR = 0.0005\n",
      "Epoch 58 | Train Loss: 0.028203 | Test Loss: 0.025802\n",
      "Epoch 60: Current LR = 0.0005\n",
      "Epoch 59 | Train Loss: 0.026518 | Test Loss: 0.023880\n",
      "Epoch 61: Current LR = 0.0005\n",
      "Epoch 60 | Train Loss: 0.024608 | Test Loss: 0.021820\n",
      "Epoch 62: Current LR = 0.0005\n",
      "Epoch 61 | Train Loss: 0.022533 | Test Loss: 0.019771\n",
      "Epoch 63: Current LR = 0.0005\n",
      "Epoch 62 | Train Loss: 0.020484 | Test Loss: 0.017805\n",
      "Epoch 64: Current LR = 0.0005\n",
      "Epoch 63 | Train Loss: 0.018505 | Test Loss: 0.015926\n",
      "Epoch 65: Current LR = 0.0005\n",
      "Epoch 64 | Train Loss: 0.016620 | Test Loss: 0.014199\n",
      "Epoch 66: Current LR = 0.0005\n",
      "Epoch 65 | Train Loss: 0.014885 | Test Loss: 0.012806\n",
      "Epoch 67: Current LR = 0.0005\n",
      "Epoch 66 | Train Loss: 0.013476 | Test Loss: 0.011883\n",
      "Epoch 68: Current LR = 0.0005\n",
      "Epoch 67 | Train Loss: 0.012512 | Test Loss: 0.011528\n",
      "Epoch 69: Current LR = 0.0005\n",
      "Epoch 68 | Train Loss: 0.012128 | Test Loss: 0.011903\n",
      "Epoch 70: Current LR = 0.0005\n",
      "Epoch 69 | Train Loss: 0.012423 | Test Loss: 0.012547\n",
      "Epoch 71: Current LR = 0.0005\n",
      "Epoch 70 | Train Loss: 0.012995 | Test Loss: 0.013174\n",
      "Epoch 72: Current LR = 0.0005\n",
      "Epoch 71 | Train Loss: 0.013628 | Test Loss: 0.013706\n",
      "Epoch 73: Current LR = 0.0005\n",
      "Epoch 72 | Train Loss: 0.014159 | Test Loss: 0.013874\n",
      "Epoch 74: Current LR = 0.0005\n",
      "Epoch 73 | Train Loss: 0.014304 | Test Loss: 0.013676\n",
      "Epoch 75: Current LR = 0.00025\n",
      "Epoch 74 | Train Loss: 0.014104 | Test Loss: 0.013436\n",
      "Epoch 76: Current LR = 0.00025\n",
      "Epoch 75 | Train Loss: 0.013818 | Test Loss: 0.013062\n",
      "Epoch 77: Current LR = 0.00025\n",
      "Epoch 76 | Train Loss: 0.013465 | Test Loss: 0.012569\n",
      "Epoch 78: Current LR = 0.00025\n",
      "Epoch 77 | Train Loss: 0.013001 | Test Loss: 0.012008\n",
      "Epoch 79: Current LR = 0.00025\n",
      "Epoch 78 | Train Loss: 0.012444 | Test Loss: 0.011465\n",
      "Epoch 80: Current LR = 0.00025\n",
      "Epoch 79 | Train Loss: 0.011948 | Test Loss: 0.011006\n",
      "Epoch 81: Current LR = 0.00025\n",
      "Epoch 80 | Train Loss: 0.011506 | Test Loss: 0.010649\n",
      "Epoch 82: Current LR = 0.00025\n",
      "Epoch 81 | Train Loss: 0.011189 | Test Loss: 0.010392\n",
      "Epoch 83: Current LR = 0.00025\n",
      "Epoch 82 | Train Loss: 0.010938 | Test Loss: 0.010246\n",
      "Epoch 84: Current LR = 0.00025\n",
      "Epoch 83 | Train Loss: 0.010804 | Test Loss: 0.010165\n",
      "Epoch 85: Current LR = 0.00025\n",
      "Epoch 84 | Train Loss: 0.010730 | Test Loss: 0.010135\n",
      "Epoch 86: Current LR = 0.00025\n",
      "Epoch 85 | Train Loss: 0.010732 | Test Loss: 0.010122\n",
      "Epoch 87: Current LR = 0.00025\n",
      "Epoch 86 | Train Loss: 0.010736 | Test Loss: 0.010103\n",
      "Epoch 88: Current LR = 0.00025\n",
      "Epoch 87 | Train Loss: 0.010690 | Test Loss: 0.010067\n",
      "Epoch 89: Current LR = 0.00025\n",
      "Epoch 88 | Train Loss: 0.010665 | Test Loss: 0.009998\n",
      "Epoch 90: Current LR = 0.00025\n",
      "Epoch 89 | Train Loss: 0.010596 | Test Loss: 0.009908\n",
      "Epoch 91: Current LR = 0.00025\n",
      "Epoch 90 | Train Loss: 0.010514 | Test Loss: 0.009802\n",
      "Epoch 92: Current LR = 0.00025\n",
      "Epoch 91 | Train Loss: 0.010392 | Test Loss: 0.009681\n",
      "Epoch 93: Current LR = 0.00025\n",
      "Epoch 92 | Train Loss: 0.010253 | Test Loss: 0.009565\n",
      "Epoch 94: Current LR = 0.00025\n",
      "Epoch 93 | Train Loss: 0.010137 | Test Loss: 0.009458\n",
      "Epoch 95: Current LR = 0.00025\n",
      "Epoch 94 | Train Loss: 0.010021 | Test Loss: 0.009367\n",
      "Epoch 96: Current LR = 0.00025\n",
      "Epoch 95 | Train Loss: 0.009912 | Test Loss: 0.009305\n",
      "Epoch 97: Current LR = 0.00025\n",
      "Epoch 96 | Train Loss: 0.009868 | Test Loss: 0.009265\n",
      "Epoch 98: Current LR = 0.00025\n",
      "Epoch 97 | Train Loss: 0.009798 | Test Loss: 0.009212\n",
      "Epoch 99: Current LR = 0.00025\n",
      "Epoch 98 | Train Loss: 0.009755 | Test Loss: 0.009155\n",
      "Epoch 100: Current LR = 0.00025\n",
      "Epoch 99 | Train Loss: 0.009682 | Test Loss: 0.009089\n",
      "Epoch 101: Current LR = 0.00025\n",
      "Epoch 100 | Train Loss: 0.009607 | Test Loss: 0.009014\n",
      "Epoch 102: Current LR = 0.00025\n",
      "Epoch 101 | Train Loss: 0.009530 | Test Loss: 0.008916\n",
      "Epoch 103: Current LR = 0.00025\n",
      "Epoch 102 | Train Loss: 0.009453 | Test Loss: 0.008823\n",
      "Epoch 104: Current LR = 0.00025\n",
      "Epoch 103 | Train Loss: 0.009355 | Test Loss: 0.008743\n",
      "Epoch 105: Current LR = 0.00025\n",
      "Epoch 104 | Train Loss: 0.009282 | Test Loss: 0.008666\n",
      "Epoch 106: Current LR = 0.00025\n",
      "Epoch 105 | Train Loss: 0.009214 | Test Loss: 0.008604\n",
      "Epoch 107: Current LR = 0.00025\n",
      "Epoch 106 | Train Loss: 0.009151 | Test Loss: 0.008540\n",
      "Epoch 108: Current LR = 0.00025\n",
      "Epoch 107 | Train Loss: 0.009062 | Test Loss: 0.008475\n",
      "Epoch 109: Current LR = 0.00025\n",
      "Epoch 108 | Train Loss: 0.009038 | Test Loss: 0.008411\n",
      "Epoch 110: Current LR = 0.00025\n",
      "Epoch 109 | Train Loss: 0.008954 | Test Loss: 0.008346\n",
      "Epoch 111: Current LR = 0.00025\n",
      "Epoch 110 | Train Loss: 0.008893 | Test Loss: 0.008296\n",
      "Epoch 112: Current LR = 0.00025\n",
      "Epoch 111 | Train Loss: 0.008801 | Test Loss: 0.008246\n",
      "Epoch 113: Current LR = 0.00025\n",
      "Epoch 112 | Train Loss: 0.008779 | Test Loss: 0.008207\n",
      "Epoch 114: Current LR = 0.00025\n",
      "Epoch 113 | Train Loss: 0.008729 | Test Loss: 0.008171\n",
      "Epoch 115: Current LR = 0.00025\n",
      "Epoch 114 | Train Loss: 0.008681 | Test Loss: 0.008118\n",
      "Epoch 116: Current LR = 0.00025\n",
      "Epoch 115 | Train Loss: 0.008622 | Test Loss: 0.008069\n",
      "Epoch 117: Current LR = 0.00025\n",
      "Epoch 116 | Train Loss: 0.008593 | Test Loss: 0.008028\n",
      "Epoch 118: Current LR = 0.00025\n",
      "Epoch 117 | Train Loss: 0.008549 | Test Loss: 0.007978\n",
      "Epoch 119: Current LR = 0.00025\n",
      "Epoch 118 | Train Loss: 0.008510 | Test Loss: 0.007941\n",
      "Epoch 120: Current LR = 0.00025\n",
      "Epoch 119 | Train Loss: 0.008475 | Test Loss: 0.007903\n",
      "Epoch 121: Current LR = 0.00025\n",
      "Epoch 120 | Train Loss: 0.008415 | Test Loss: 0.007862\n",
      "Epoch 122: Current LR = 0.00025\n",
      "Epoch 121 | Train Loss: 0.008382 | Test Loss: 0.007831\n",
      "Epoch 123: Current LR = 0.00025\n",
      "Epoch 122 | Train Loss: 0.008383 | Test Loss: 0.007795\n",
      "Epoch 124: Current LR = 0.00025\n",
      "Epoch 123 | Train Loss: 0.008322 | Test Loss: 0.007770\n",
      "Epoch 125: Current LR = 0.00025\n",
      "Epoch 124 | Train Loss: 0.008295 | Test Loss: 0.007737\n",
      "Epoch 126: Current LR = 0.00025\n",
      "Epoch 125 | Train Loss: 0.008253 | Test Loss: 0.007712\n",
      "Epoch 127: Current LR = 0.00025\n",
      "Epoch 126 | Train Loss: 0.008255 | Test Loss: 0.007683\n",
      "Epoch 128: Current LR = 0.00025\n",
      "Epoch 127 | Train Loss: 0.008228 | Test Loss: 0.007652\n",
      "Epoch 129: Current LR = 0.00025\n",
      "Epoch 128 | Train Loss: 0.008188 | Test Loss: 0.007622\n",
      "Epoch 130: Current LR = 0.00025\n",
      "Epoch 129 | Train Loss: 0.008157 | Test Loss: 0.007607\n",
      "Epoch 131: Current LR = 0.00025\n",
      "Epoch 130 | Train Loss: 0.008119 | Test Loss: 0.007587\n",
      "Epoch 132: Current LR = 0.00025\n",
      "Epoch 131 | Train Loss: 0.008092 | Test Loss: 0.007555\n",
      "Epoch 133: Current LR = 0.00025\n",
      "Epoch 132 | Train Loss: 0.008096 | Test Loss: 0.007535\n",
      "Epoch 134: Current LR = 0.00025\n",
      "Epoch 133 | Train Loss: 0.008051 | Test Loss: 0.007525\n",
      "Epoch 135: Current LR = 0.00025\n",
      "Epoch 134 | Train Loss: 0.008030 | Test Loss: 0.007500\n",
      "Epoch 136: Current LR = 0.00025\n",
      "Epoch 135 | Train Loss: 0.008016 | Test Loss: 0.007498\n",
      "Epoch 137: Current LR = 0.00025\n",
      "Epoch 136 | Train Loss: 0.008048 | Test Loss: 0.007493\n",
      "Epoch 138: Current LR = 0.00025\n",
      "Epoch 137 | Train Loss: 0.008012 | Test Loss: 0.007449\n",
      "Epoch 139: Current LR = 0.00025\n",
      "Epoch 138 | Train Loss: 0.007973 | Test Loss: 0.007431\n",
      "Epoch 140: Current LR = 0.00025\n",
      "Epoch 139 | Train Loss: 0.007939 | Test Loss: 0.007425\n",
      "Epoch 141: Current LR = 0.00025\n",
      "Epoch 140 | Train Loss: 0.007932 | Test Loss: 0.007390\n",
      "Epoch 142: Current LR = 0.00025\n",
      "Epoch 141 | Train Loss: 0.007914 | Test Loss: 0.007399\n",
      "Epoch 143: Current LR = 0.00025\n",
      "Epoch 142 | Train Loss: 0.007927 | Test Loss: 0.007414\n",
      "Epoch 144: Current LR = 0.00025\n",
      "Epoch 143 | Train Loss: 0.007942 | Test Loss: 0.007370\n",
      "Epoch 145: Current LR = 0.00025\n",
      "Epoch 144 | Train Loss: 0.007892 | Test Loss: 0.007319\n",
      "Epoch 146: Current LR = 0.00025\n",
      "Epoch 145 | Train Loss: 0.007851 | Test Loss: 0.007395\n",
      "Epoch 147: Current LR = 0.00025\n",
      "Epoch 146 | Train Loss: 0.007889 | Test Loss: 0.007490\n",
      "Epoch 148: Current LR = 0.00025\n",
      "Epoch 147 | Train Loss: 0.007966 | Test Loss: 0.007479\n",
      "Epoch 149: Current LR = 0.00025\n",
      "Epoch 148 | Train Loss: 0.007961 | Test Loss: 0.007368\n",
      "Epoch 150: Current LR = 0.00025\n",
      "Epoch 149 | Train Loss: 0.007871 | Test Loss: 0.007264\n",
      "Epoch 151: Current LR = 0.00025\n",
      "Epoch 150 | Train Loss: 0.007776 | Test Loss: 0.007323\n",
      "Epoch 152: Current LR = 0.00025\n",
      "Epoch 151 | Train Loss: 0.007852 | Test Loss: 0.007433\n",
      "Epoch 153: Current LR = 0.00025\n",
      "Epoch 152 | Train Loss: 0.008000 | Test Loss: 0.007444\n",
      "Epoch 154: Current LR = 0.00025\n",
      "Epoch 153 | Train Loss: 0.008013 | Test Loss: 0.007347\n",
      "Epoch 155: Current LR = 0.00025\n",
      "Epoch 154 | Train Loss: 0.007879 | Test Loss: 0.007234\n",
      "Epoch 156: Current LR = 0.00025\n",
      "Epoch 155 | Train Loss: 0.007768 | Test Loss: 0.007267\n",
      "Epoch 157: Current LR = 0.00025\n",
      "Epoch 156 | Train Loss: 0.007782 | Test Loss: 0.007371\n",
      "Epoch 158: Current LR = 0.00025\n",
      "Epoch 157 | Train Loss: 0.007884 | Test Loss: 0.007381\n",
      "Epoch 159: Current LR = 0.00025\n",
      "Epoch 158 | Train Loss: 0.007874 | Test Loss: 0.007290\n",
      "Epoch 160: Current LR = 0.00025\n",
      "Epoch 159 | Train Loss: 0.007782 | Test Loss: 0.007198\n",
      "Epoch 161: Current LR = 0.00025\n",
      "Epoch 160 | Train Loss: 0.007714 | Test Loss: 0.007288\n",
      "Epoch 162: Current LR = 0.00025\n",
      "Epoch 161 | Train Loss: 0.007815 | Test Loss: 0.007453\n",
      "Epoch 163: Current LR = 0.00025\n",
      "Epoch 162 | Train Loss: 0.008017 | Test Loss: 0.007506\n",
      "Epoch 164: Current LR = 0.00025\n",
      "Epoch 163 | Train Loss: 0.008071 | Test Loss: 0.007411\n",
      "Epoch 165: Current LR = 0.00025\n",
      "Epoch 164 | Train Loss: 0.007966 | Test Loss: 0.007244\n",
      "Epoch 166: Current LR = 0.00025\n",
      "Epoch 165 | Train Loss: 0.007765 | Test Loss: 0.007153\n",
      "Epoch 167: Current LR = 0.00025\n",
      "Epoch 166 | Train Loss: 0.007703 | Test Loss: 0.007279\n",
      "Epoch 168: Current LR = 0.00025\n",
      "Epoch 167 | Train Loss: 0.007797 | Test Loss: 0.007437\n",
      "Epoch 169: Current LR = 0.00025\n",
      "Epoch 168 | Train Loss: 0.007923 | Test Loss: 0.007460\n",
      "Epoch 170: Current LR = 0.00025\n",
      "Epoch 169 | Train Loss: 0.007959 | Test Loss: 0.007338\n",
      "Epoch 171: Current LR = 0.00025\n",
      "Epoch 170 | Train Loss: 0.007828 | Test Loss: 0.007172\n",
      "Epoch 172: Current LR = 0.00025\n",
      "Epoch 171 | Train Loss: 0.007697 | Test Loss: 0.007136\n",
      "Epoch 173: Current LR = 0.00025\n",
      "Epoch 172 | Train Loss: 0.007653 | Test Loss: 0.007216\n",
      "Epoch 174: Current LR = 0.00025\n",
      "Epoch 173 | Train Loss: 0.007730 | Test Loss: 0.007237\n",
      "Epoch 175: Current LR = 0.00025\n",
      "Epoch 174 | Train Loss: 0.007773 | Test Loss: 0.007174\n",
      "Epoch 176: Current LR = 0.00025\n",
      "Epoch 175 | Train Loss: 0.007710 | Test Loss: 0.007103\n",
      "Epoch 177: Current LR = 0.00025\n",
      "Epoch 176 | Train Loss: 0.007642 | Test Loss: 0.007171\n",
      "Epoch 178: Current LR = 0.00025\n",
      "Epoch 177 | Train Loss: 0.007682 | Test Loss: 0.007270\n",
      "Epoch 179: Current LR = 0.00025\n",
      "Epoch 178 | Train Loss: 0.007766 | Test Loss: 0.007260\n",
      "Epoch 180: Current LR = 0.00025\n",
      "Epoch 179 | Train Loss: 0.007778 | Test Loss: 0.007156\n",
      "Epoch 181: Current LR = 0.00025\n",
      "Epoch 180 | Train Loss: 0.007685 | Test Loss: 0.007074\n",
      "Epoch 182: Current LR = 0.00025\n",
      "Epoch 181 | Train Loss: 0.007591 | Test Loss: 0.007084\n",
      "Epoch 183: Current LR = 0.00025\n",
      "Epoch 182 | Train Loss: 0.007632 | Test Loss: 0.007078\n",
      "Epoch 184: Current LR = 0.00025\n",
      "Epoch 183 | Train Loss: 0.007627 | Test Loss: 0.007050\n",
      "Epoch 185: Current LR = 0.00025\n",
      "Epoch 184 | Train Loss: 0.007588 | Test Loss: 0.007099\n",
      "Epoch 186: Current LR = 0.00025\n",
      "Epoch 185 | Train Loss: 0.007621 | Test Loss: 0.007146\n",
      "Epoch 187: Current LR = 0.00025\n",
      "Epoch 186 | Train Loss: 0.007652 | Test Loss: 0.007108\n",
      "Epoch 188: Current LR = 0.00025\n",
      "Epoch 187 | Train Loss: 0.007615 | Test Loss: 0.007034\n",
      "Epoch 189: Current LR = 0.00025\n",
      "Epoch 188 | Train Loss: 0.007560 | Test Loss: 0.007055\n",
      "Epoch 190: Current LR = 0.00025\n",
      "Epoch 189 | Train Loss: 0.007618 | Test Loss: 0.007114\n",
      "Epoch 191: Current LR = 0.00025\n",
      "Epoch 190 | Train Loss: 0.007658 | Test Loss: 0.007096\n",
      "Epoch 192: Current LR = 0.00025\n",
      "Epoch 191 | Train Loss: 0.007662 | Test Loss: 0.007024\n",
      "Epoch 193: Current LR = 0.00025\n",
      "Epoch 192 | Train Loss: 0.007562 | Test Loss: 0.007018\n",
      "Epoch 194: Current LR = 0.00025\n",
      "Epoch 193 | Train Loss: 0.007551 | Test Loss: 0.007057\n",
      "Epoch 195: Current LR = 0.00025\n",
      "Epoch 194 | Train Loss: 0.007576 | Test Loss: 0.007036\n",
      "Epoch 196: Current LR = 0.00025\n",
      "Epoch 195 | Train Loss: 0.007555 | Test Loss: 0.006990\n",
      "Epoch 197: Current LR = 0.00025\n",
      "Epoch 196 | Train Loss: 0.007511 | Test Loss: 0.007044\n",
      "Epoch 198: Current LR = 0.00025\n",
      "Epoch 197 | Train Loss: 0.007587 | Test Loss: 0.007116\n",
      "Epoch 199: Current LR = 0.00025\n",
      "Epoch 198 | Train Loss: 0.007663 | Test Loss: 0.007095\n",
      "Epoch 200: Current LR = 0.00025\n",
      "Epoch 199 | Train Loss: 0.007631 | Test Loss: 0.007003\n",
      "Epoch 201: Current LR = 0.00025\n",
      "Epoch 200 | Train Loss: 0.007547 | Test Loss: 0.006960\n",
      "Epoch 202: Current LR = 0.00025\n",
      "Epoch 201 | Train Loss: 0.007465 | Test Loss: 0.006979\n",
      "Epoch 203: Current LR = 0.00025\n",
      "Epoch 202 | Train Loss: 0.007492 | Test Loss: 0.006958\n",
      "Epoch 204: Current LR = 0.00025\n",
      "Epoch 203 | Train Loss: 0.007486 | Test Loss: 0.006940\n",
      "Epoch 205: Current LR = 0.00025\n",
      "Epoch 204 | Train Loss: 0.007493 | Test Loss: 0.006938\n",
      "Epoch 206: Current LR = 0.00025\n",
      "Epoch 205 | Train Loss: 0.007479 | Test Loss: 0.006924\n",
      "Epoch 207: Current LR = 0.00025\n",
      "Epoch 206 | Train Loss: 0.007474 | Test Loss: 0.006963\n",
      "Epoch 208: Current LR = 0.00025\n",
      "Epoch 207 | Train Loss: 0.007497 | Test Loss: 0.006975\n",
      "Epoch 209: Current LR = 0.00025\n",
      "Epoch 208 | Train Loss: 0.007485 | Test Loss: 0.006925\n",
      "Epoch 210: Current LR = 0.00025\n",
      "Epoch 209 | Train Loss: 0.007447 | Test Loss: 0.006898\n",
      "Epoch 211: Current LR = 0.00025\n",
      "Epoch 210 | Train Loss: 0.007451 | Test Loss: 0.006907\n",
      "Epoch 212: Current LR = 0.00025\n",
      "Epoch 211 | Train Loss: 0.007467 | Test Loss: 0.006885\n",
      "Epoch 213: Current LR = 0.00025\n",
      "Epoch 212 | Train Loss: 0.007414 | Test Loss: 0.006892\n",
      "Epoch 214: Current LR = 0.00025\n",
      "Epoch 213 | Train Loss: 0.007455 | Test Loss: 0.006900\n",
      "Epoch 215: Current LR = 0.00025\n",
      "Epoch 214 | Train Loss: 0.007434 | Test Loss: 0.006868\n",
      "Epoch 216: Current LR = 0.00025\n",
      "Epoch 215 | Train Loss: 0.007409 | Test Loss: 0.006881\n",
      "Epoch 217: Current LR = 0.00025\n",
      "Epoch 216 | Train Loss: 0.007422 | Test Loss: 0.006903\n",
      "Epoch 218: Current LR = 0.00025\n",
      "Epoch 217 | Train Loss: 0.007433 | Test Loss: 0.006869\n",
      "Epoch 219: Current LR = 0.00025\n",
      "Epoch 218 | Train Loss: 0.007438 | Test Loss: 0.006844\n",
      "Epoch 220: Current LR = 0.00025\n",
      "Epoch 219 | Train Loss: 0.007370 | Test Loss: 0.006845\n",
      "Epoch 221: Current LR = 0.00025\n",
      "Epoch 220 | Train Loss: 0.007385 | Test Loss: 0.006830\n",
      "Epoch 222: Current LR = 0.00025\n",
      "Epoch 221 | Train Loss: 0.007358 | Test Loss: 0.006824\n",
      "Epoch 223: Current LR = 0.00025\n",
      "Epoch 222 | Train Loss: 0.007364 | Test Loss: 0.006816\n",
      "Epoch 224: Current LR = 0.00025\n",
      "Epoch 223 | Train Loss: 0.007356 | Test Loss: 0.006803\n",
      "Epoch 225: Current LR = 0.00025\n",
      "Epoch 224 | Train Loss: 0.007326 | Test Loss: 0.006827\n",
      "Epoch 226: Current LR = 0.00025\n",
      "Epoch 225 | Train Loss: 0.007385 | Test Loss: 0.006828\n",
      "Epoch 227: Current LR = 0.00025\n",
      "Epoch 226 | Train Loss: 0.007364 | Test Loss: 0.006782\n",
      "Epoch 228: Current LR = 0.00025\n",
      "Epoch 227 | Train Loss: 0.007334 | Test Loss: 0.006796\n",
      "Epoch 229: Current LR = 0.00025\n",
      "Epoch 228 | Train Loss: 0.007347 | Test Loss: 0.006827\n",
      "Epoch 230: Current LR = 0.00025\n",
      "Epoch 229 | Train Loss: 0.007381 | Test Loss: 0.006790\n",
      "Epoch 231: Current LR = 0.00025\n",
      "Epoch 230 | Train Loss: 0.007345 | Test Loss: 0.006745\n",
      "Epoch 232: Current LR = 0.00025\n",
      "Epoch 231 | Train Loss: 0.007295 | Test Loss: 0.006849\n",
      "Epoch 233: Current LR = 0.00025\n",
      "Epoch 232 | Train Loss: 0.007349 | Test Loss: 0.006962\n",
      "Epoch 234: Current LR = 0.00025\n",
      "Epoch 233 | Train Loss: 0.007490 | Test Loss: 0.006939\n",
      "Epoch 235: Current LR = 0.00025\n",
      "Epoch 234 | Train Loss: 0.007464 | Test Loss: 0.006808\n",
      "Epoch 236: Current LR = 0.00025\n",
      "Epoch 235 | Train Loss: 0.007334 | Test Loss: 0.006714\n",
      "Epoch 237: Current LR = 0.00025\n",
      "Epoch 236 | Train Loss: 0.007269 | Test Loss: 0.006726\n",
      "Epoch 238: Current LR = 0.00025\n",
      "Epoch 237 | Train Loss: 0.007284 | Test Loss: 0.006713\n",
      "Epoch 239: Current LR = 0.00025\n",
      "Epoch 238 | Train Loss: 0.007251 | Test Loss: 0.006703\n",
      "Epoch 240: Current LR = 0.00025\n",
      "Epoch 239 | Train Loss: 0.007246 | Test Loss: 0.006698\n",
      "Epoch 241: Current LR = 0.00025\n",
      "Epoch 240 | Train Loss: 0.007238 | Test Loss: 0.006691\n",
      "Epoch 242: Current LR = 0.00025\n",
      "Epoch 241 | Train Loss: 0.007222 | Test Loss: 0.006684\n",
      "Epoch 243: Current LR = 0.00025\n",
      "Epoch 242 | Train Loss: 0.007240 | Test Loss: 0.006687\n",
      "Epoch 244: Current LR = 0.00025\n",
      "Epoch 243 | Train Loss: 0.007246 | Test Loss: 0.006681\n",
      "Epoch 245: Current LR = 0.00025\n",
      "Epoch 244 | Train Loss: 0.007224 | Test Loss: 0.006668\n",
      "Epoch 246: Current LR = 0.00025\n",
      "Epoch 245 | Train Loss: 0.007203 | Test Loss: 0.006662\n",
      "Epoch 247: Current LR = 0.00025\n",
      "Epoch 246 | Train Loss: 0.007208 | Test Loss: 0.006668\n",
      "Epoch 248: Current LR = 0.00025\n",
      "Epoch 247 | Train Loss: 0.007216 | Test Loss: 0.006661\n",
      "Epoch 249: Current LR = 0.00025\n",
      "Epoch 248 | Train Loss: 0.007185 | Test Loss: 0.006649\n",
      "Epoch 250: Current LR = 0.00025\n",
      "Epoch 249 | Train Loss: 0.007180 | Test Loss: 0.006644\n",
      "Epoch 251: Current LR = 0.00025\n",
      "Epoch 250 | Train Loss: 0.007176 | Test Loss: 0.006646\n",
      "Epoch 252: Current LR = 0.00025\n",
      "Epoch 251 | Train Loss: 0.007169 | Test Loss: 0.006639\n",
      "Epoch 253: Current LR = 0.00025\n",
      "Epoch 252 | Train Loss: 0.007193 | Test Loss: 0.006633\n",
      "Epoch 254: Current LR = 0.00025\n",
      "Epoch 253 | Train Loss: 0.007170 | Test Loss: 0.006629\n",
      "Epoch 255: Current LR = 0.00025\n",
      "Epoch 254 | Train Loss: 0.007179 | Test Loss: 0.006623\n",
      "Epoch 256: Current LR = 0.00025\n",
      "Epoch 255 | Train Loss: 0.007142 | Test Loss: 0.006615\n",
      "Epoch 257: Current LR = 0.00025\n",
      "Epoch 256 | Train Loss: 0.007136 | Test Loss: 0.006622\n",
      "Epoch 258: Current LR = 0.00025\n",
      "Epoch 257 | Train Loss: 0.007145 | Test Loss: 0.006617\n",
      "Epoch 259: Current LR = 0.00025\n",
      "Epoch 258 | Train Loss: 0.007155 | Test Loss: 0.006597\n",
      "Epoch 260: Current LR = 0.00025\n",
      "Epoch 259 | Train Loss: 0.007118 | Test Loss: 0.006604\n",
      "Epoch 261: Current LR = 0.00025\n",
      "Epoch 260 | Train Loss: 0.007158 | Test Loss: 0.006583\n",
      "Epoch 262: Current LR = 0.00025\n",
      "Epoch 261 | Train Loss: 0.007116 | Test Loss: 0.006592\n",
      "Epoch 263: Current LR = 0.00025\n",
      "Epoch 262 | Train Loss: 0.007131 | Test Loss: 0.006598\n",
      "Epoch 264: Current LR = 0.00025\n",
      "Epoch 263 | Train Loss: 0.007154 | Test Loss: 0.006560\n",
      "Epoch 265: Current LR = 0.00025\n",
      "Epoch 264 | Train Loss: 0.007093 | Test Loss: 0.006574\n",
      "Epoch 266: Current LR = 0.00025\n",
      "Epoch 265 | Train Loss: 0.007114 | Test Loss: 0.006594\n",
      "Epoch 267: Current LR = 0.00025\n",
      "Epoch 266 | Train Loss: 0.007110 | Test Loss: 0.006551\n",
      "Epoch 268: Current LR = 0.00025\n",
      "Epoch 267 | Train Loss: 0.007071 | Test Loss: 0.006535\n",
      "Epoch 269: Current LR = 0.00025\n",
      "Epoch 268 | Train Loss: 0.007089 | Test Loss: 0.006546\n",
      "Epoch 270: Current LR = 0.00025\n",
      "Epoch 269 | Train Loss: 0.007077 | Test Loss: 0.006518\n",
      "Epoch 271: Current LR = 0.00025\n",
      "Epoch 270 | Train Loss: 0.007062 | Test Loss: 0.006538\n",
      "Epoch 272: Current LR = 0.00025\n",
      "Epoch 271 | Train Loss: 0.007080 | Test Loss: 0.006558\n",
      "Epoch 273: Current LR = 0.00025\n",
      "Epoch 272 | Train Loss: 0.007080 | Test Loss: 0.006517\n",
      "Epoch 274: Current LR = 0.00025\n",
      "Epoch 273 | Train Loss: 0.007052 | Test Loss: 0.006505\n",
      "Epoch 275: Current LR = 0.00025\n",
      "Epoch 274 | Train Loss: 0.007031 | Test Loss: 0.006518\n",
      "Epoch 276: Current LR = 0.00025\n",
      "Epoch 275 | Train Loss: 0.007075 | Test Loss: 0.006491\n",
      "Epoch 277: Current LR = 0.00025\n",
      "Epoch 276 | Train Loss: 0.007027 | Test Loss: 0.006512\n",
      "Epoch 278: Current LR = 0.00025\n",
      "Epoch 277 | Train Loss: 0.007038 | Test Loss: 0.006535\n",
      "Epoch 279: Current LR = 0.00025\n",
      "Epoch 278 | Train Loss: 0.007066 | Test Loss: 0.006495\n",
      "Epoch 280: Current LR = 0.00025\n",
      "Epoch 279 | Train Loss: 0.007016 | Test Loss: 0.006483\n",
      "Epoch 281: Current LR = 0.00025\n",
      "Epoch 280 | Train Loss: 0.007034 | Test Loss: 0.006497\n",
      "Epoch 282: Current LR = 0.00025\n",
      "Epoch 281 | Train Loss: 0.007036 | Test Loss: 0.006471\n",
      "Epoch 283: Current LR = 0.00025\n",
      "Epoch 282 | Train Loss: 0.007013 | Test Loss: 0.006490\n",
      "Epoch 284: Current LR = 0.00025\n",
      "Epoch 283 | Train Loss: 0.007029 | Test Loss: 0.006510\n",
      "Epoch 285: Current LR = 0.00025\n",
      "Epoch 284 | Train Loss: 0.007019 | Test Loss: 0.006471\n",
      "Epoch 286: Current LR = 0.00025\n",
      "Epoch 285 | Train Loss: 0.006989 | Test Loss: 0.006469\n",
      "Epoch 287: Current LR = 0.00025\n",
      "Epoch 286 | Train Loss: 0.007006 | Test Loss: 0.006488\n",
      "Epoch 288: Current LR = 0.00025\n",
      "Epoch 287 | Train Loss: 0.007037 | Test Loss: 0.006458\n",
      "Epoch 289: Current LR = 0.00025\n",
      "Epoch 288 | Train Loss: 0.007008 | Test Loss: 0.006461\n",
      "Epoch 290: Current LR = 0.00025\n",
      "Epoch 289 | Train Loss: 0.006974 | Test Loss: 0.006476\n",
      "Epoch 291: Current LR = 0.00025\n",
      "Epoch 290 | Train Loss: 0.006995 | Test Loss: 0.006445\n",
      "Epoch 292: Current LR = 0.00025\n",
      "Epoch 291 | Train Loss: 0.006972 | Test Loss: 0.006464\n",
      "Epoch 293: Current LR = 0.00025\n",
      "Epoch 292 | Train Loss: 0.007016 | Test Loss: 0.006490\n",
      "Epoch 294: Current LR = 0.00025\n",
      "Epoch 293 | Train Loss: 0.007047 | Test Loss: 0.006453\n",
      "Epoch 295: Current LR = 0.00025\n",
      "Epoch 294 | Train Loss: 0.006974 | Test Loss: 0.006433\n",
      "Epoch 296: Current LR = 0.00025\n",
      "Epoch 295 | Train Loss: 0.006946 | Test Loss: 0.006442\n",
      "Epoch 297: Current LR = 0.00025\n",
      "Epoch 296 | Train Loss: 0.006953 | Test Loss: 0.006421\n",
      "Epoch 298: Current LR = 0.00025\n",
      "Epoch 297 | Train Loss: 0.006968 | Test Loss: 0.006455\n",
      "Epoch 299: Current LR = 0.00025\n",
      "Epoch 298 | Train Loss: 0.006978 | Test Loss: 0.006475\n",
      "Epoch 300: Current LR = 0.00025\n",
      "Epoch 299 | Train Loss: 0.007019 | Test Loss: 0.006428\n",
      "Epoch 301: Current LR = 0.00025\n",
      "Epoch 300 | Train Loss: 0.006980 | Test Loss: 0.006408\n",
      "Epoch 302: Current LR = 0.00025\n",
      "Epoch 301 | Train Loss: 0.006927 | Test Loss: 0.006419\n",
      "Epoch 303: Current LR = 0.00025\n",
      "Epoch 302 | Train Loss: 0.006919 | Test Loss: 0.006393\n",
      "Epoch 304: Current LR = 0.00025\n",
      "Epoch 303 | Train Loss: 0.006937 | Test Loss: 0.006421\n",
      "Epoch 305: Current LR = 0.00025\n",
      "Epoch 304 | Train Loss: 0.006973 | Test Loss: 0.006447\n",
      "Epoch 306: Current LR = 0.00025\n",
      "Epoch 305 | Train Loss: 0.006993 | Test Loss: 0.006403\n",
      "Epoch 307: Current LR = 0.00025\n",
      "Epoch 306 | Train Loss: 0.006932 | Test Loss: 0.006374\n",
      "Epoch 308: Current LR = 0.00025\n",
      "Epoch 307 | Train Loss: 0.006883 | Test Loss: 0.006380\n",
      "Epoch 309: Current LR = 0.00025\n",
      "Epoch 308 | Train Loss: 0.006913 | Test Loss: 0.006361\n",
      "Epoch 310: Current LR = 0.00025\n",
      "Epoch 309 | Train Loss: 0.006873 | Test Loss: 0.006352\n",
      "Epoch 311: Current LR = 0.00025\n",
      "Epoch 310 | Train Loss: 0.006851 | Test Loss: 0.006353\n",
      "Epoch 312: Current LR = 0.00025\n",
      "Epoch 311 | Train Loss: 0.006860 | Test Loss: 0.006331\n",
      "Epoch 313: Current LR = 0.00025\n",
      "Epoch 312 | Train Loss: 0.006854 | Test Loss: 0.006350\n",
      "Epoch 314: Current LR = 0.00025\n",
      "Epoch 313 | Train Loss: 0.006879 | Test Loss: 0.006360\n",
      "Epoch 315: Current LR = 0.00025\n",
      "Epoch 314 | Train Loss: 0.006902 | Test Loss: 0.006311\n",
      "Epoch 316: Current LR = 0.00025\n",
      "Epoch 315 | Train Loss: 0.006852 | Test Loss: 0.006300\n",
      "Epoch 317: Current LR = 0.00025\n",
      "Epoch 316 | Train Loss: 0.006821 | Test Loss: 0.006315\n",
      "Epoch 318: Current LR = 0.00025\n",
      "Epoch 317 | Train Loss: 0.006839 | Test Loss: 0.006279\n",
      "Epoch 319: Current LR = 0.00025\n",
      "Epoch 318 | Train Loss: 0.006783 | Test Loss: 0.006278\n",
      "Epoch 320: Current LR = 0.00025\n",
      "Epoch 319 | Train Loss: 0.006810 | Test Loss: 0.006289\n",
      "Epoch 321: Current LR = 0.00025\n",
      "Epoch 320 | Train Loss: 0.006832 | Test Loss: 0.006253\n",
      "Epoch 322: Current LR = 0.00025\n",
      "Epoch 321 | Train Loss: 0.006782 | Test Loss: 0.006270\n",
      "Epoch 323: Current LR = 0.00025\n",
      "Epoch 322 | Train Loss: 0.006770 | Test Loss: 0.006295\n",
      "Epoch 324: Current LR = 0.00025\n",
      "Epoch 323 | Train Loss: 0.006814 | Test Loss: 0.006252\n",
      "Epoch 325: Current LR = 0.00025\n",
      "Epoch 324 | Train Loss: 0.006756 | Test Loss: 0.006232\n",
      "Epoch 326: Current LR = 0.00025\n",
      "Epoch 325 | Train Loss: 0.006768 | Test Loss: 0.006242\n",
      "Epoch 327: Current LR = 0.00025\n",
      "Epoch 326 | Train Loss: 0.006769 | Test Loss: 0.006217\n",
      "Epoch 328: Current LR = 0.00025\n",
      "Epoch 327 | Train Loss: 0.006748 | Test Loss: 0.006247\n",
      "Epoch 329: Current LR = 0.00025\n",
      "Epoch 328 | Train Loss: 0.006767 | Test Loss: 0.006269\n",
      "Epoch 330: Current LR = 0.00025\n",
      "Epoch 329 | Train Loss: 0.006769 | Test Loss: 0.006222\n",
      "Epoch 331: Current LR = 0.00025\n",
      "Epoch 330 | Train Loss: 0.006751 | Test Loss: 0.006207\n",
      "Epoch 332: Current LR = 0.00025\n",
      "Epoch 331 | Train Loss: 0.006722 | Test Loss: 0.006225\n",
      "Epoch 333: Current LR = 0.00025\n",
      "Epoch 332 | Train Loss: 0.006756 | Test Loss: 0.006195\n",
      "Epoch 334: Current LR = 0.00025\n",
      "Epoch 333 | Train Loss: 0.006722 | Test Loss: 0.006204\n",
      "Epoch 335: Current LR = 0.00025\n",
      "Epoch 334 | Train Loss: 0.006723 | Test Loss: 0.006218\n",
      "Epoch 336: Current LR = 0.00025\n",
      "Epoch 335 | Train Loss: 0.006756 | Test Loss: 0.006183\n",
      "Epoch 337: Current LR = 0.00025\n",
      "Epoch 336 | Train Loss: 0.006687 | Test Loss: 0.006199\n",
      "Epoch 338: Current LR = 0.00025\n",
      "Epoch 337 | Train Loss: 0.006724 | Test Loss: 0.006226\n",
      "Epoch 339: Current LR = 0.00025\n",
      "Epoch 338 | Train Loss: 0.006755 | Test Loss: 0.006186\n",
      "Epoch 340: Current LR = 0.00025\n",
      "Epoch 339 | Train Loss: 0.006709 | Test Loss: 0.006168\n",
      "Epoch 341: Current LR = 0.00025\n",
      "Epoch 340 | Train Loss: 0.006691 | Test Loss: 0.006179\n",
      "Epoch 342: Current LR = 0.00025\n",
      "Epoch 341 | Train Loss: 0.006692 | Test Loss: 0.006154\n",
      "Epoch 343: Current LR = 0.00025\n",
      "Epoch 342 | Train Loss: 0.006677 | Test Loss: 0.006193\n",
      "Epoch 344: Current LR = 0.00025\n",
      "Epoch 343 | Train Loss: 0.006749 | Test Loss: 0.006225\n",
      "Epoch 345: Current LR = 0.00025\n",
      "Epoch 344 | Train Loss: 0.006755 | Test Loss: 0.006175\n",
      "Epoch 346: Current LR = 0.00025\n",
      "Epoch 345 | Train Loss: 0.006707 | Test Loss: 0.006137\n",
      "Epoch 347: Current LR = 0.00025\n",
      "Epoch 346 | Train Loss: 0.006667 | Test Loss: 0.006141\n",
      "Epoch 348: Current LR = 0.00025\n",
      "Epoch 347 | Train Loss: 0.006651 | Test Loss: 0.006123\n",
      "Epoch 349: Current LR = 0.00025\n",
      "Epoch 348 | Train Loss: 0.006652 | Test Loss: 0.006119\n",
      "Epoch 350: Current LR = 0.00025\n",
      "Epoch 349 | Train Loss: 0.006631 | Test Loss: 0.006113\n",
      "Epoch 351: Current LR = 0.00025\n",
      "Epoch 350 | Train Loss: 0.006635 | Test Loss: 0.006102\n",
      "Epoch 352: Current LR = 0.00025\n",
      "Epoch 351 | Train Loss: 0.006630 | Test Loss: 0.006128\n",
      "Epoch 353: Current LR = 0.00025\n",
      "Epoch 352 | Train Loss: 0.006637 | Test Loss: 0.006129\n",
      "Epoch 354: Current LR = 0.00025\n",
      "Epoch 353 | Train Loss: 0.006645 | Test Loss: 0.006083\n",
      "Epoch 355: Current LR = 0.00025\n",
      "Epoch 354 | Train Loss: 0.006594 | Test Loss: 0.006124\n",
      "Epoch 356: Current LR = 0.00025\n",
      "Epoch 355 | Train Loss: 0.006654 | Test Loss: 0.006173\n",
      "Epoch 357: Current LR = 0.00025\n",
      "Epoch 356 | Train Loss: 0.006704 | Test Loss: 0.006125\n",
      "Epoch 358: Current LR = 0.00025\n",
      "Epoch 357 | Train Loss: 0.006646 | Test Loss: 0.006054\n",
      "Epoch 359: Current LR = 0.00025\n",
      "Epoch 358 | Train Loss: 0.006576 | Test Loss: 0.006155\n",
      "Epoch 360: Current LR = 0.00025\n",
      "Epoch 359 | Train Loss: 0.006671 | Test Loss: 0.006284\n",
      "Epoch 361: Current LR = 0.00025\n",
      "Epoch 360 | Train Loss: 0.006792 | Test Loss: 0.006260\n",
      "Epoch 362: Current LR = 0.00025\n",
      "Epoch 361 | Train Loss: 0.006755 | Test Loss: 0.006115\n",
      "Epoch 363: Current LR = 0.00025\n",
      "Epoch 362 | Train Loss: 0.006617 | Test Loss: 0.006027\n",
      "Epoch 364: Current LR = 0.00025\n",
      "Epoch 363 | Train Loss: 0.006506 | Test Loss: 0.006050\n",
      "Epoch 365: Current LR = 0.00025\n",
      "Epoch 364 | Train Loss: 0.006589 | Test Loss: 0.006032\n",
      "Epoch 366: Current LR = 0.00025\n",
      "Epoch 365 | Train Loss: 0.006552 | Test Loss: 0.006015\n",
      "Epoch 367: Current LR = 0.00025\n",
      "Epoch 366 | Train Loss: 0.006537 | Test Loss: 0.006013\n",
      "Epoch 368: Current LR = 0.00025\n",
      "Epoch 367 | Train Loss: 0.006523 | Test Loss: 0.006004\n",
      "Epoch 369: Current LR = 0.00025\n",
      "Epoch 368 | Train Loss: 0.006536 | Test Loss: 0.005996\n",
      "Epoch 370: Current LR = 0.00025\n",
      "Epoch 369 | Train Loss: 0.006521 | Test Loss: 0.006010\n",
      "Epoch 371: Current LR = 0.00025\n",
      "Epoch 370 | Train Loss: 0.006519 | Test Loss: 0.006005\n",
      "Epoch 372: Current LR = 0.00025\n",
      "Epoch 371 | Train Loss: 0.006535 | Test Loss: 0.005978\n",
      "Epoch 373: Current LR = 0.00025\n",
      "Epoch 372 | Train Loss: 0.006491 | Test Loss: 0.005972\n",
      "Epoch 374: Current LR = 0.00025\n",
      "Epoch 373 | Train Loss: 0.006483 | Test Loss: 0.005989\n",
      "Epoch 375: Current LR = 0.00025\n",
      "Epoch 374 | Train Loss: 0.006497 | Test Loss: 0.005977\n",
      "Epoch 376: Current LR = 0.00025\n",
      "Epoch 375 | Train Loss: 0.006492 | Test Loss: 0.005952\n",
      "Epoch 377: Current LR = 0.00025\n",
      "Epoch 376 | Train Loss: 0.006482 | Test Loss: 0.005946\n",
      "Epoch 378: Current LR = 0.00025\n",
      "Epoch 377 | Train Loss: 0.006461 | Test Loss: 0.005949\n",
      "Epoch 379: Current LR = 0.00025\n",
      "Epoch 378 | Train Loss: 0.006448 | Test Loss: 0.005936\n",
      "Epoch 380: Current LR = 0.00025\n",
      "Epoch 379 | Train Loss: 0.006449 | Test Loss: 0.005934\n",
      "Epoch 381: Current LR = 0.00025\n",
      "Epoch 380 | Train Loss: 0.006462 | Test Loss: 0.005929\n",
      "Epoch 382: Current LR = 0.00025\n",
      "Epoch 381 | Train Loss: 0.006455 | Test Loss: 0.005911\n",
      "Epoch 383: Current LR = 0.00025\n",
      "Epoch 382 | Train Loss: 0.006421 | Test Loss: 0.005901\n",
      "Epoch 384: Current LR = 0.00025\n",
      "Epoch 383 | Train Loss: 0.006405 | Test Loss: 0.005887\n",
      "Epoch 385: Current LR = 0.00025\n",
      "Epoch 384 | Train Loss: 0.006405 | Test Loss: 0.005880\n",
      "Epoch 386: Current LR = 0.00025\n",
      "Epoch 385 | Train Loss: 0.006402 | Test Loss: 0.005859\n",
      "Epoch 387: Current LR = 0.00025\n",
      "Epoch 386 | Train Loss: 0.006343 | Test Loss: 0.005845\n",
      "Epoch 388: Current LR = 0.00025\n",
      "Epoch 387 | Train Loss: 0.006354 | Test Loss: 0.005831\n",
      "Epoch 389: Current LR = 0.00025\n",
      "Epoch 388 | Train Loss: 0.006353 | Test Loss: 0.005810\n",
      "Epoch 390: Current LR = 0.00025\n",
      "Epoch 389 | Train Loss: 0.006321 | Test Loss: 0.005829\n",
      "Epoch 391: Current LR = 0.00025\n",
      "Epoch 390 | Train Loss: 0.006326 | Test Loss: 0.005827\n",
      "Epoch 392: Current LR = 0.00025\n",
      "Epoch 391 | Train Loss: 0.006326 | Test Loss: 0.005773\n",
      "Epoch 393: Current LR = 0.00025\n",
      "Epoch 392 | Train Loss: 0.006294 | Test Loss: 0.005802\n",
      "Epoch 394: Current LR = 0.00025\n",
      "Epoch 393 | Train Loss: 0.006316 | Test Loss: 0.005832\n",
      "Epoch 395: Current LR = 0.00025\n",
      "Epoch 394 | Train Loss: 0.006375 | Test Loss: 0.005771\n",
      "Epoch 396: Current LR = 0.00025\n",
      "Epoch 395 | Train Loss: 0.006323 | Test Loss: 0.005732\n",
      "Epoch 397: Current LR = 0.00025\n",
      "Epoch 396 | Train Loss: 0.006242 | Test Loss: 0.005739\n",
      "Epoch 398: Current LR = 0.00025\n",
      "Epoch 397 | Train Loss: 0.006265 | Test Loss: 0.005708\n",
      "Epoch 399: Current LR = 0.00025\n",
      "Epoch 398 | Train Loss: 0.006233 | Test Loss: 0.005766\n",
      "Epoch 400: Current LR = 0.00025\n",
      "Epoch 399 | Train Loss: 0.006280 | Test Loss: 0.005815\n",
      "Epoch 401: Current LR = 0.00025\n",
      "Epoch 400 | Train Loss: 0.006344 | Test Loss: 0.005746\n",
      "Epoch 402: Current LR = 0.00025\n",
      "Epoch 401 | Train Loss: 0.006268 | Test Loss: 0.005661\n",
      "Epoch 403: Current LR = 0.00025\n",
      "Epoch 402 | Train Loss: 0.006163 | Test Loss: 0.005812\n",
      "Epoch 404: Current LR = 0.00025\n",
      "Epoch 403 | Train Loss: 0.006330 | Test Loss: 0.005997\n",
      "Epoch 405: Current LR = 0.00025\n",
      "Epoch 404 | Train Loss: 0.006472 | Test Loss: 0.005977\n",
      "Epoch 406: Current LR = 0.00025\n",
      "Epoch 405 | Train Loss: 0.006453 | Test Loss: 0.005780\n",
      "Epoch 407: Current LR = 0.00025\n",
      "Epoch 406 | Train Loss: 0.006262 | Test Loss: 0.005617\n",
      "Epoch 408: Current LR = 0.00025\n",
      "Epoch 407 | Train Loss: 0.006139 | Test Loss: 0.005808\n",
      "Epoch 409: Current LR = 0.00025\n",
      "Epoch 408 | Train Loss: 0.006338 | Test Loss: 0.006101\n",
      "Epoch 410: Current LR = 0.00025\n",
      "Epoch 409 | Train Loss: 0.006645 | Test Loss: 0.006169\n",
      "Epoch 411: Current LR = 0.00025\n",
      "Epoch 410 | Train Loss: 0.006701 | Test Loss: 0.005975\n",
      "Epoch 412: Current LR = 0.00025\n",
      "Epoch 411 | Train Loss: 0.006506 | Test Loss: 0.005679\n",
      "Epoch 413: Current LR = 0.00025\n",
      "Epoch 412 | Train Loss: 0.006219 | Test Loss: 0.005589\n",
      "Epoch 414: Current LR = 0.00025\n",
      "Epoch 413 | Train Loss: 0.006103 | Test Loss: 0.005697\n",
      "Epoch 415: Current LR = 0.00025\n",
      "Epoch 414 | Train Loss: 0.006198 | Test Loss: 0.005705\n",
      "Epoch 416: Current LR = 0.00025\n",
      "Epoch 415 | Train Loss: 0.006212 | Test Loss: 0.005597\n",
      "Epoch 417: Current LR = 0.00025\n",
      "Epoch 416 | Train Loss: 0.006093 | Test Loss: 0.005564\n",
      "Epoch 418: Current LR = 0.00025\n",
      "Epoch 417 | Train Loss: 0.006086 | Test Loss: 0.005606\n",
      "Epoch 419: Current LR = 0.00025\n",
      "Epoch 418 | Train Loss: 0.006124 | Test Loss: 0.005569\n",
      "Epoch 420: Current LR = 0.00025\n",
      "Epoch 419 | Train Loss: 0.006060 | Test Loss: 0.005537\n",
      "Epoch 421: Current LR = 0.00025\n",
      "Epoch 420 | Train Loss: 0.006042 | Test Loss: 0.005539\n",
      "Epoch 422: Current LR = 0.00025\n",
      "Epoch 421 | Train Loss: 0.006033 | Test Loss: 0.005522\n",
      "Epoch 423: Current LR = 0.00025\n",
      "Epoch 422 | Train Loss: 0.006054 | Test Loss: 0.005514\n",
      "Epoch 424: Current LR = 0.00025\n",
      "Epoch 423 | Train Loss: 0.006037 | Test Loss: 0.005544\n",
      "Epoch 425: Current LR = 0.00025\n",
      "Epoch 424 | Train Loss: 0.006042 | Test Loss: 0.005529\n",
      "Epoch 426: Current LR = 0.00025\n",
      "Epoch 425 | Train Loss: 0.006018 | Test Loss: 0.005485\n",
      "Epoch 427: Current LR = 0.00025\n",
      "Epoch 426 | Train Loss: 0.005982 | Test Loss: 0.005475\n",
      "Epoch 428: Current LR = 0.00025\n",
      "Epoch 427 | Train Loss: 0.005996 | Test Loss: 0.005487\n",
      "Epoch 429: Current LR = 0.00025\n",
      "Epoch 428 | Train Loss: 0.005975 | Test Loss: 0.005468\n",
      "Epoch 430: Current LR = 0.00025\n",
      "Epoch 429 | Train Loss: 0.005972 | Test Loss: 0.005452\n",
      "Epoch 431: Current LR = 0.00025\n",
      "Epoch 430 | Train Loss: 0.005935 | Test Loss: 0.005445\n",
      "Epoch 432: Current LR = 0.00025\n",
      "Epoch 431 | Train Loss: 0.005944 | Test Loss: 0.005427\n",
      "Epoch 433: Current LR = 0.00025\n",
      "Epoch 432 | Train Loss: 0.005950 | Test Loss: 0.005414\n",
      "Epoch 434: Current LR = 0.00025\n",
      "Epoch 433 | Train Loss: 0.005906 | Test Loss: 0.005428\n",
      "Epoch 435: Current LR = 0.00025\n",
      "Epoch 434 | Train Loss: 0.005963 | Test Loss: 0.005413\n",
      "Epoch 436: Current LR = 0.00025\n",
      "Epoch 435 | Train Loss: 0.005915 | Test Loss: 0.005375\n",
      "Epoch 437: Current LR = 0.00025\n",
      "Epoch 436 | Train Loss: 0.005866 | Test Loss: 0.005359\n",
      "Epoch 438: Current LR = 0.00025\n",
      "Epoch 437 | Train Loss: 0.005860 | Test Loss: 0.005338\n",
      "Epoch 439: Current LR = 0.00025\n",
      "Epoch 438 | Train Loss: 0.005828 | Test Loss: 0.005318\n",
      "Epoch 440: Current LR = 0.00025\n",
      "Epoch 439 | Train Loss: 0.005821 | Test Loss: 0.005292\n",
      "Epoch 441: Current LR = 0.00025\n",
      "Epoch 440 | Train Loss: 0.005792 | Test Loss: 0.005285\n",
      "Epoch 442: Current LR = 0.00025\n",
      "Epoch 441 | Train Loss: 0.005795 | Test Loss: 0.005250\n",
      "Epoch 443: Current LR = 0.00025\n",
      "Epoch 442 | Train Loss: 0.005758 | Test Loss: 0.005302\n",
      "Epoch 444: Current LR = 0.00025\n",
      "Epoch 443 | Train Loss: 0.005810 | Test Loss: 0.005332\n",
      "Epoch 445: Current LR = 0.00025\n",
      "Epoch 444 | Train Loss: 0.005873 | Test Loss: 0.005228\n",
      "Epoch 446: Current LR = 0.00025\n",
      "Epoch 445 | Train Loss: 0.005756 | Test Loss: 0.005167\n",
      "Epoch 447: Current LR = 0.00025\n",
      "Epoch 446 | Train Loss: 0.005688 | Test Loss: 0.005183\n",
      "Epoch 448: Current LR = 0.00025\n",
      "Epoch 447 | Train Loss: 0.005701 | Test Loss: 0.005133\n",
      "Epoch 449: Current LR = 0.00025\n",
      "Epoch 448 | Train Loss: 0.005644 | Test Loss: 0.005139\n",
      "Epoch 450: Current LR = 0.00025\n",
      "Epoch 449 | Train Loss: 0.005670 | Test Loss: 0.005141\n",
      "Epoch 451: Current LR = 0.00025\n",
      "Epoch 450 | Train Loss: 0.005672 | Test Loss: 0.005080\n",
      "Epoch 452: Current LR = 0.00025\n",
      "Epoch 451 | Train Loss: 0.005601 | Test Loss: 0.005192\n",
      "Epoch 453: Current LR = 0.00025\n",
      "Epoch 452 | Train Loss: 0.005698 | Test Loss: 0.005294\n",
      "Epoch 454: Current LR = 0.00025\n",
      "Epoch 453 | Train Loss: 0.005793 | Test Loss: 0.005196\n",
      "Epoch 455: Current LR = 0.00025\n",
      "Epoch 454 | Train Loss: 0.005720 | Test Loss: 0.005044\n",
      "Epoch 456: Current LR = 0.00025\n",
      "Epoch 455 | Train Loss: 0.005566 | Test Loss: 0.005263\n",
      "Epoch 457: Current LR = 0.00025\n",
      "Epoch 456 | Train Loss: 0.005793 | Test Loss: 0.005591\n",
      "Epoch 458: Current LR = 0.00025\n",
      "Epoch 457 | Train Loss: 0.006150 | Test Loss: 0.005622\n",
      "Epoch 459: Current LR = 0.00025\n",
      "Epoch 458 | Train Loss: 0.006170 | Test Loss: 0.005342\n",
      "Epoch 460: Current LR = 0.00025\n",
      "Epoch 459 | Train Loss: 0.005875 | Test Loss: 0.005029\n",
      "Epoch 461: Current LR = 0.00025\n",
      "Epoch 460 | Train Loss: 0.005581 | Test Loss: 0.005160\n",
      "Epoch 462: Current LR = 0.00025\n",
      "Epoch 461 | Train Loss: 0.005682 | Test Loss: 0.005515\n",
      "Epoch 463: Current LR = 0.00025\n",
      "Epoch 462 | Train Loss: 0.006008 | Test Loss: 0.005600\n",
      "Epoch 464: Current LR = 0.00025\n",
      "Epoch 463 | Train Loss: 0.006106 | Test Loss: 0.005355\n",
      "Epoch 465: Current LR = 0.00025\n",
      "Epoch 464 | Train Loss: 0.005877 | Test Loss: 0.005029\n",
      "Epoch 466: Current LR = 0.00025\n",
      "Epoch 465 | Train Loss: 0.005555 | Test Loss: 0.005070\n",
      "Epoch 467: Current LR = 0.000125\n",
      "Epoch 466 | Train Loss: 0.005605 | Test Loss: 0.005172\n",
      "Epoch 468: Current LR = 0.000125\n",
      "Epoch 467 | Train Loss: 0.005726 | Test Loss: 0.005177\n",
      "Epoch 469: Current LR = 0.000125\n",
      "Epoch 468 | Train Loss: 0.005708 | Test Loss: 0.005089\n",
      "Epoch 470: Current LR = 0.000125\n",
      "Epoch 469 | Train Loss: 0.005640 | Test Loss: 0.004983\n",
      "Epoch 471: Current LR = 0.000125\n",
      "Epoch 470 | Train Loss: 0.005520 | Test Loss: 0.004986\n",
      "Epoch 472: Current LR = 0.000125\n",
      "Epoch 471 | Train Loss: 0.005509 | Test Loss: 0.005064\n",
      "Epoch 473: Current LR = 0.000125\n",
      "Epoch 472 | Train Loss: 0.005592 | Test Loss: 0.005088\n",
      "Epoch 474: Current LR = 0.000125\n",
      "Epoch 473 | Train Loss: 0.005615 | Test Loss: 0.005036\n",
      "Epoch 475: Current LR = 0.000125\n",
      "Epoch 474 | Train Loss: 0.005550 | Test Loss: 0.004965\n",
      "Epoch 476: Current LR = 0.000125\n",
      "Epoch 475 | Train Loss: 0.005489 | Test Loss: 0.004985\n",
      "Epoch 477: Current LR = 0.000125\n",
      "Epoch 476 | Train Loss: 0.005514 | Test Loss: 0.005052\n",
      "Epoch 478: Current LR = 0.000125\n",
      "Epoch 477 | Train Loss: 0.005570 | Test Loss: 0.005059\n",
      "Epoch 479: Current LR = 0.000125\n",
      "Epoch 478 | Train Loss: 0.005595 | Test Loss: 0.005004\n",
      "Epoch 480: Current LR = 0.000125\n",
      "Epoch 479 | Train Loss: 0.005545 | Test Loss: 0.004951\n",
      "Epoch 481: Current LR = 0.000125\n",
      "Epoch 480 | Train Loss: 0.005460 | Test Loss: 0.005012\n",
      "Epoch 482: Current LR = 0.000125\n",
      "Epoch 481 | Train Loss: 0.005563 | Test Loss: 0.005099\n",
      "Epoch 483: Current LR = 0.000125\n",
      "Epoch 482 | Train Loss: 0.005576 | Test Loss: 0.005108\n",
      "Epoch 484: Current LR = 0.000125\n",
      "Epoch 483 | Train Loss: 0.005612 | Test Loss: 0.005035\n",
      "Epoch 485: Current LR = 0.000125\n",
      "Epoch 484 | Train Loss: 0.005537 | Test Loss: 0.004953\n",
      "Epoch 486: Current LR = 0.000125\n",
      "Epoch 485 | Train Loss: 0.005475 | Test Loss: 0.004978\n",
      "Epoch 487: Current LR = 6.25e-05\n",
      "Epoch 486 | Train Loss: 0.005504 | Test Loss: 0.005013\n",
      "Epoch 488: Current LR = 6.25e-05\n",
      "Epoch 487 | Train Loss: 0.005543 | Test Loss: 0.005021\n",
      "Epoch 489: Current LR = 6.25e-05\n",
      "Epoch 488 | Train Loss: 0.005564 | Test Loss: 0.004998\n",
      "Epoch 490: Current LR = 6.25e-05\n",
      "Epoch 489 | Train Loss: 0.005526 | Test Loss: 0.004961\n",
      "Epoch 491: Current LR = 6.25e-05\n",
      "Epoch 490 | Train Loss: 0.005487 | Test Loss: 0.004938\n",
      "Epoch 492: Current LR = 6.25e-05\n",
      "Epoch 491 | Train Loss: 0.005471 | Test Loss: 0.004966\n",
      "Epoch 493: Current LR = 6.25e-05\n",
      "Epoch 492 | Train Loss: 0.005483 | Test Loss: 0.005012\n",
      "Epoch 494: Current LR = 6.25e-05\n",
      "Epoch 493 | Train Loss: 0.005537 | Test Loss: 0.005036\n",
      "Epoch 495: Current LR = 6.25e-05\n",
      "Epoch 494 | Train Loss: 0.005553 | Test Loss: 0.005023\n",
      "Epoch 496: Current LR = 6.25e-05\n",
      "Epoch 495 | Train Loss: 0.005549 | Test Loss: 0.004985\n",
      "Epoch 497: Current LR = 6.25e-05\n",
      "Epoch 496 | Train Loss: 0.005487 | Test Loss: 0.004944\n",
      "Epoch 498: Current LR = 3.125e-05\n",
      "Epoch 497 | Train Loss: 0.005464 | Test Loss: 0.004933\n",
      "Epoch 499: Current LR = 3.125e-05\n",
      "Epoch 498 | Train Loss: 0.005439 | Test Loss: 0.004937\n",
      "Epoch 500: Current LR = 3.125e-05\n",
      "Epoch 499 | Train Loss: 0.005461 | Test Loss: 0.004949\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集 \n",
    "dataset = StockDataset(bk_data_ls, seq_length=16, forecast_gap=2)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_set, test_set = torch.utils.data.random_split( \n",
    "    dataset, [train_size, len(dataset) - train_size]\n",
    ")\n",
    "\n",
    "# 数据加载器 \n",
    "train_loader = DataLoader(train_set, batch_size=100000, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=100000)\n",
    "\n",
    "# 模型初始化\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 加载模型（需先实例化结构）\n",
    "model = MultiScaleAttentionLSTM(\n",
    "    input_size=7,  # 7个特征 \n",
    "    hidden_size=64,\n",
    "    num_layers_long = 3, \n",
    "    num_layers_short = 2\n",
    ").to(device)\n",
    "\n",
    "if os.path.exists('./model/model.pth'):\n",
    "    model.load_state_dict(torch.load('./model/model.pth')) \n",
    "\n",
    "# 训练参数 \n",
    "criterion = nn.MSELoss()\n",
    "scaler = amp.GradScaler(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "    optimizer, \n",
    "    mode='min',       # 监控验证损失\n",
    "    factor=0.5,       # 学习率衰减系数 \n",
    "    patience=5,       # 容忍5个epoch无改善\n",
    ")\n",
    "\n",
    "# ===== 4. 训练循环 =====\n",
    "for epoch in range(500):\n",
    "    model.train() \n",
    "    train_loss = 0 \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device),  y_batch.to(device) \n",
    "        \n",
    "        with amp.autocast(device_type=device): \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        # 反向传播（自动处理精度）\n",
    "        scaler.scale(loss).backward()   # 缩放梯度\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)          # 更新参数 \n",
    "        scaler.update()                 # 调整缩放因子 \n",
    "        optimizer.zero_grad() \n",
    "    print(f\"Epoch {epoch+1}: Current LR = {scheduler.get_last_lr()[0]}\") \n",
    "    # 验证\n",
    "    model.eval() \n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device),  y_test.to(device) \n",
    "            preds = model(X_test)\n",
    "            test_loss += criterion(preds, y_test).item()\n",
    "    \n",
    "    scheduler.step(test_loss) \n",
    "    print(f'Epoch {epoch} | Train Loss: {train_loss/len(train_loader):.6f} | Test Loss: {test_loss/len(test_loader):.6f}')\n",
    "\n",
    "torch.save(model.state_dict(),  './model/model.pth') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca4f939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('互联网电商', 881177, 8720.552, 8420.463, 3.563809)\n"
     ]
    }
   ],
   "source": [
    "# ===== 5. 预测示例 =====\n",
    "model.eval()\n",
    "dtype = [('name', 'U20'), ('code', 'i4'), ('real_pred', 'f4'), ('close', 'f4'), ('change_rate', 'f4')]\n",
    "arr = np.empty(len(bk_data_ls), dtype= dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(bk_data_ls)):\n",
    "        name, code, data = bk_data_ls[i]\n",
    "        scaler = dataset.get_scaler(code)\n",
    "        max_index = len(data) - dataset.seq_length\n",
    "        seq_features = scaler.transform(data.iloc[max_index:,1:])\n",
    "        code_feature = code_scaler.transform(pd.DataFrame({\"code\": [code]}))\n",
    "        seq_features = np.insert(seq_features,  1, code_feature, axis=1)\n",
    "        X = torch.tensor(seq_features,  dtype=torch.float32).to(device)\n",
    "        X=X.unsqueeze(0)  # 添加批次维度\n",
    "\n",
    "        prediction = model(X)\n",
    "        scaled_pred = prediction.cpu().numpy() \n",
    "        \n",
    "        # 逆归一化收盘价 \n",
    "        dummy = np.zeros((1,  6))\n",
    "        dummy[:, 3] = scaled_pred  # 将预测值放入close列\n",
    "        real_pred = scaler.inverse_transform(dummy)[0,  3]\n",
    "        # if real_pred > data.iloc[-1,  3]:\n",
    "        #     print(\"high\")\n",
    "        # else:\n",
    "        #     print(\"low\")\n",
    "        # print(f'stock name: {name}, stock code: {code}, 今天收盘价：{data.iloc[-1,  3]}, 预测的后天收盘价: {real_pred:.2f}')\n",
    "        change_rate = (real_pred/data.iloc[-1,  3] - 1)* 100\n",
    "        arr[i] = (name, code, real_pred, data.iloc[-1,  3], change_rate)\n",
    "\n",
    "arr = arr[np.argsort(arr['change_rate'])][::-1]\n",
    "print(arr[0])\n",
    "np.savetxt(\"output.csv\",  arr, delimiter=\",\", fmt='%s,%d,%.4f,%.4f,%.4f', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c8eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b84f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ed238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
